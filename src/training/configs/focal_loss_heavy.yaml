# Focal Loss Heavy Configuration
# For cases with severe class imbalance

model:
  name: "OnlineTransformer"
  d_model: 512
  n_heads: 8
  n_layers: 6
  sequence_length: 256
  dropout: 0.1
  vocab_size: 4665

# Pure focal loss approach
loss:
  type: "focal"
  focal_alpha: 0.25    # Standard alpha for rare class focus
  focal_gamma: 2.5     # Higher gamma for stronger imbalance correction
  label_smoothing: 0.05 # Light smoothing
  ignore_index: 177

training:
  batch_size: 32
  learning_rate: 4.0e-4
  weight_decay: 0.01
  max_epochs: 45
  warmup_steps: 800
  grad_clip_norm: 1.0
  
  scheduler:
    type: "cosine_with_warmup"
    warmup_ratio: 0.1
    min_lr_ratio: 0.1

data:
  data_dir: "data/interim"
  sequence_length: 256
  mode: "online"
  num_workers: 4
  pin_memory: true

evaluation:
  eval_every: 500
  eval_steps: 100
  save_best: true
  
  diversity_metrics:
    enabled: true
    log_every: 100
    vocab_coverage_threshold: 0.12
    dominance_threshold: 0.28

sampling:
  strategy: "nucleus"
  temperature: 1.3
  top_p: 0.9
  repetition_penalty: 1.15
  frequency_penalty: 0.08

logging:
  wandb:
    project: "martydepth-balanced"
    name: "focal_heavy"
  log_every: 50
  
checkpointing:
  save_every: 1000
  keep_last: 3 