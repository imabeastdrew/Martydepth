# Balanced Training Configuration - Base
# Addresses class imbalance and diversity issues

model:
  name: "OnlineTransformer"
  d_model: 512
  n_heads: 8
  n_layers: 6
  sequence_length: 256
  dropout: 0.1
  vocab_size: 4665  # Will be set automatically

# Loss function configuration
loss:
  type: "weighted_cross_entropy"  # Options: cross_entropy, weighted_cross_entropy, focal, mixed
  weight_method: "sqrt_inverse"   # Options: inverse_frequency, sqrt_inverse, log_inverse
  label_smoothing: 0.1
  ignore_index: 177  # Pad token

# Training hyperparameters  
training:
  batch_size: 32
  learning_rate: 5.0e-4  # Lower than before to prevent overfitting
  weight_decay: 0.01
  max_epochs: 50
  warmup_steps: 1000
  grad_clip_norm: 1.0
  
  # Learning rate scheduling
  scheduler:
    type: "cosine_with_warmup"
    warmup_ratio: 0.1
    min_lr_ratio: 0.1

# Data configuration
data:
  data_dir: "data/interim"
  sequence_length: 256
  mode: "online"
  num_workers: 4
  pin_memory: true

# Evaluation during training
evaluation:
  eval_every: 500  # steps
  eval_steps: 100
  save_best: true
  
  # Diversity monitoring
  diversity_metrics:
    enabled: true
    log_every: 100
    vocab_coverage_threshold: 0.1
    dominance_threshold: 0.3

# Sampling for evaluation
sampling:
  strategy: "nucleus"
  temperature: 1.2
  top_p: 0.9
  repetition_penalty: 1.1
  frequency_penalty: 0.05

# Logging and checkpointing
logging:
  wandb:
    project: "martydepth-balanced"
    name: "balanced_base"
  log_every: 50
  
checkpointing:
  save_every: 1000  # steps
  keep_last: 3 