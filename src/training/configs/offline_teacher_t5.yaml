# Configuration for T5OfflineTeacherModel - Direct comparison with custom model
# NOTE: T5 outputs full vocabulary but loss calculated on chord tokens only + targets adjusted to chord space

# --- Model Selection ---
model_type: t5  # Use T5OfflineTeacherModel

# --- Data settings ---
data_dir: data/interim
batch_size: 128
num_workers: 2

# --- Model architecture (SAME AS CUSTOM for fair comparison) ---
embed_dim: 512
num_layers: 4  # 4 encoder + 4 decoder = 8 total layers
num_heads: 8
max_sequence_length: 256
dropout: 0.2  # Increased dropout to fight overfitting

# --- Training loop (ANTI-OVERFITTING) ---
max_epochs: 15  # Shorter training with early stopping
learning_rate: 1.0e-5  # Keep stable learning rate
weight_decay: 0.05  # Stronger L2 regularization
warmup_steps: 100  # Minimal warmup
gradient_clip_val: 0.5  # Keep tight clipping
log_every_n_steps: 50

# Early stopping configuration (implement in training script)
early_stopping_patience: 5  # Stop if no improvement for 5 epochs
early_stopping_min_delta: 0.001  # Minimum improvement threshold

# --- W&B Logging ---
wandb_project: "martydepth"
checkpoint_dir: /work/10539/drewtaylor635/vista/Martydepth/checkpoints 