# Configuration for Contrastive Reward Model Training

# Data args
data_dir: "data/interim"
max_seq_length: 512

# Model args
embed_dim: 512
num_heads: 8
num_layers: 6
dropout: 0.1

# Training args
epochs: 35
batch_size: 128
learning_rate: 1.0e-4
temperature: 0.07
num_workers: 4

# W&B args
wandb_project: "martydepth"
checkpoint_dir: /work/10539/drewtaylor635/vista/Martydepth/checkpoints 