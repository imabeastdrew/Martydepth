# Diversity Regularized Training Configuration
# Uses mixed loss with strong diversity penalties

model:
  name: "OnlineTransformer"
  d_model: 512
  n_heads: 8
  n_layers: 6
  sequence_length: 256
  dropout: 0.1
  vocab_size: 4665

# Mixed loss with diversity regularization
loss:
  type: "mixed"
  use_weighted_base: true
  weight_method: "sqrt_inverse"
  label_smoothing: 0.1
  ignore_index: 177
  
  # Diversity regularization weights
  diversity_weight: 0.2      # Strong diversity penalty
  focal_weight: 0.1          # Moderate focal loss
  contrastive_weight: 0.1    # Contrastive diversity
  adaptive_weighting: true   # Adapt weights during training

training:
  batch_size: 24  # Smaller batch for more diverse gradients
  learning_rate: 3.0e-4  # Even lower learning rate
  weight_decay: 0.015
  max_epochs: 60
  warmup_steps: 1500
  grad_clip_norm: 0.8  # Tighter gradient clipping
  
  scheduler:
    type: "cosine_with_warmup"
    warmup_ratio: 0.15  # Longer warmup
    min_lr_ratio: 0.05

data:
  data_dir: "data/interim"
  sequence_length: 256
  mode: "online"
  num_workers: 4
  pin_memory: true

evaluation:
  eval_every: 300  # More frequent evaluation
  eval_steps: 150
  save_best: true
  
  diversity_metrics:
    enabled: true
    log_every: 50   # More frequent diversity monitoring
    vocab_coverage_threshold: 0.15  # Higher threshold
    dominance_threshold: 0.25       # Lower dominance threshold

# More diverse sampling
sampling:
  strategy: "nucleus"
  temperature: 1.5  # Higher temperature
  top_p: 0.85       # Slightly lower p
  repetition_penalty: 1.2
  frequency_penalty: 0.1
  presence_penalty: 0.05

logging:
  wandb:
    project: "martydepth-balanced"
    name: "diversity_regularized"
  log_every: 25  # More frequent logging
  
checkpointing:
  save_every: 750
  keep_last: 5 