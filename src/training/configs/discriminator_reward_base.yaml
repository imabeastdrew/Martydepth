# Configuration for Discriminative Reward Model Training

# Model type
model_type: "discriminative"

# Data args
data_dir: "data/interim"
max_seq_length: 256  # Maximum sequence length for full-scale model

# Model args
embed_dim: 512
num_heads: 8
num_layers: 6
dropout: 0.1
scale_factor: 1.0  # Scale factor for fragment length (1.0 = full length)

# Training args
epochs: 10
batch_size: 64
learning_rate: 1.0e-4
num_workers: 4

# W&B args
wandb_project: "martydepth"
checkpoint_dir: /work/10539/drewtaylor635/vista/Martydepth/checkpoints 