# Base configuration for the OfflineTeacherModel

# --- Data settings ---
data_dir: data/interim
batch_size: 128
num_workers: 2

# --- Model architecture (Paper Spec: 8 layers total) ---
embed_dim: 512
num_layers: 4  # 4 encoder + 4 decoder = 8 total layers
num_heads: 8
max_sequence_length: 256
dropout: 0.1

# --- Training loop (Paper Spec) ---
learning_rate: 5.0e-4  # Good starting point for AdamW with T5
weight_decay: 0.01  # Added for AdamW - helps with regularization
max_epochs: 100
warmup_steps: 500
gradient_clip_val: 1.0
log_every_n_steps: 100

# --- W&B Logging ---
wandb_project: "martydepth"
checkpoint_dir: /work/10539/drewtaylor635/vista/Martydepth/checkpoints 