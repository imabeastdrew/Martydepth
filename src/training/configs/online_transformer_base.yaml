# Base configuration for the OnlineTransformer model

# --- Data settings ---
data_dir: data/interim
batch_size: 64
num_workers: 2

# --- Model architecture ---
vocab_size: 4665  # Fixed: 178 melody + 4486 chord + 1 for off-by-one error
embed_dim: 512
num_layers: 8
num_heads: 8
max_sequence_length: 256  # Note: This will be doubled internally due to interleaving
dropout: 0.1

# --- Training loop ---
learning_rate: 1.0e-3
weight_decay: 0.01
max_epochs: 50
warmup_steps: 1000
gradient_clip_val: 1.0
log_every_n_steps: 100

# --- W&B Logging ---
wandb_project: "martydepth"
checkpoint_dir: /work/10539/drewtaylor635/vista/Martydepth/checkpoints 