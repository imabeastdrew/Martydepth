{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "!git clone https://github.com/imabeastdrew/Martydepth.git\n",
        "%cd Martydepth\n",
        "%pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import torch\n",
        "import numpy as np\n",
        "import wandb\n",
        "import json\n",
        "import yaml\n",
        "from pathlib import Path\n",
        "from tqdm.notebook import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Add project root to Python path\n",
        "import sys\n",
        "sys.path.append('.')\n",
        "\n",
        "# Import project modules\n",
        "from src.data.dataset import create_dataloader\n",
        "from src.evaluation.evaluate import load_model_from_wandb as load_online_model\n",
        "from src.evaluation.evaluate_offline import load_model_from_wandb as load_offline_model\n",
        "from src.evaluation.temporal_evaluation import (\n",
        "    generate_online_temporal,\n",
        "    generate_offline_temporal,\n",
        "    calculate_test_set_baseline_temporal,\n",
        "    log_temporal_metrics,\n",
        "    create_wandb_comparison_dashboard\n",
        ")\n",
        "\n",
        "print(\"All imports successful!\")\n",
        "print(\"üìä Using pure WandB logging for temporal evaluation visualizations\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load configuration from YAML file\n",
        "config_path = 'src/evaluation/configs/temporal_evaluation.yaml'\n",
        "\n",
        "print(f\"Loading configuration from: {config_path}\")\n",
        "with open(config_path, 'r') as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "# Add notebook-specific run name\n",
        "config['run_name'] = 'temporal_evaluation_notebook'\n",
        "\n",
        "print(\"Configuration loaded from YAML:\")\n",
        "for key, value in config.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "    \n",
        "print(f\"\\nüéØ Using your configured model artifacts:\")\n",
        "print(f\"  Online:  {config['online_artifact']}\")\n",
        "print(f\"  Offline: {config['offline_artifact']}\")\n",
        "print(f\"\\nüöÄ Ready for pure WandB temporal evaluation!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup device\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"Using GPU: {torch.cuda.get_device_name()}\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Using CPU\")\n",
        "\n",
        "print(f\"Device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load models from WandB artifacts\n",
        "print(\"=== Loading Models ===\")\n",
        "\n",
        "# Load online model\n",
        "print(f\"Loading online model from: {config['online_artifact']}\")\n",
        "online_model, online_tokenizer_info, online_config = load_online_model(config['online_artifact'], device)\n",
        "online_model.eval()\n",
        "print(\"‚úÖ Online model loaded successfully\")\n",
        "\n",
        "# Load offline model  \n",
        "print(f\"Loading offline model from: {config['offline_artifact']}\")\n",
        "offline_model, offline_config, offline_tokenizer_info = load_offline_model(config['offline_artifact'], device)\n",
        "offline_model.eval()\n",
        "print(\"‚úÖ Offline model loaded successfully\")\n",
        "\n",
        "# Use online tokenizer info (should be consistent)\n",
        "tokenizer_info = online_tokenizer_info\n",
        "\n",
        "# Verify tokenizer consistency\n",
        "if online_tokenizer_info != offline_tokenizer_info:\n",
        "    print(\"‚ö†Ô∏è  WARNING: Online and offline models have different tokenizer info!\")\n",
        "    print(\"This may cause evaluation issues.\")\n",
        "else:\n",
        "    print(\"‚úÖ Tokenizer info is consistent between models\")\n",
        "\n",
        "print(f\"\\nüìä Ready to evaluate {len(config['scenarios'])} scenarios over {config['max_beats']} beats\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create data loaders\n",
        "print(\"=== Creating Data Loaders ===\")\n",
        "\n",
        "# Get sequence lengths from model configs\n",
        "online_max_seq_length = online_config.get('max_seq_length') or online_config.get('max_sequence_length') or 512\n",
        "offline_max_seq_length = offline_config.get('max_seq_length') or offline_config.get('max_sequence_length') or 256\n",
        "\n",
        "print(f\"Online model max sequence length: {online_max_seq_length}\")\n",
        "print(f\"Offline model max sequence length: {offline_max_seq_length}\")\n",
        "\n",
        "# Create online dataloader\n",
        "online_dataloader, _ = create_dataloader(\n",
        "    data_dir=Path(config['data_dir']),\n",
        "    split=config['split'],\n",
        "    batch_size=1,  # Use batch size 1 for temporal evaluation\n",
        "    num_workers=0,\n",
        "    sequence_length=online_max_seq_length,\n",
        "    mode='online',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Create offline dataloader\n",
        "offline_dataloader, _ = create_dataloader(\n",
        "    data_dir=Path(config['data_dir']),\n",
        "    split=config['split'],\n",
        "    batch_size=1,  # Use batch size 1 for temporal evaluation\n",
        "    num_workers=0,\n",
        "    sequence_length=offline_max_seq_length,\n",
        "    mode='offline',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Created dataloaders for split: '{config['split']}'\")\n",
        "print(f\"üì¶ Online dataloader: {len(online_dataloader)} batches\")\n",
        "print(f\"üì¶ Offline dataloader: {len(offline_dataloader)} batches\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate test set baseline\n",
        "print(\"=== Calculating Test Set Baseline ===\")\n",
        "baseline_results = calculate_test_set_baseline_temporal(\n",
        "    online_dataloader, tokenizer_info, max_beats=config['max_beats']\n",
        ")\n",
        "print(\"‚úÖ Baseline calculation completed\")\n",
        "\n",
        "# Run temporal evaluation for online model\n",
        "print(\"\\n=== Evaluating Online Model ===\")\n",
        "online_results = generate_online_temporal(\n",
        "    model=online_model,\n",
        "    dataloader=online_dataloader,\n",
        "    tokenizer_info=tokenizer_info,\n",
        "    device=device,\n",
        "    scenarios=config['scenarios'],\n",
        "    max_beats=config['max_beats'],\n",
        "    temperature=config['temperature'],\n",
        "    top_k=config['top_k'],\n",
        "    perturbation_beat=config['perturbation_beat']\n",
        ")\n",
        "print(\"‚úÖ Online model evaluation completed\")\n",
        "\n",
        "# Run temporal evaluation for offline model\n",
        "print(\"\\n=== Evaluating Offline Model ===\")\n",
        "offline_results = generate_offline_temporal(\n",
        "    model=offline_model,\n",
        "    dataloader=offline_dataloader,\n",
        "    tokenizer_info=tokenizer_info,\n",
        "    device=device,\n",
        "    scenarios=config['scenarios'],\n",
        "    max_beats=config['max_beats'],\n",
        "    temperature=config['temperature'],\n",
        "    top_k=config['top_k'],\n",
        "    perturbation_beat=config['perturbation_beat']\n",
        ")\n",
        "print(\"‚úÖ Offline model evaluation completed\")\n",
        "\n",
        "print(f\"\\nüéØ All evaluations completed! Ready for WandB logging.\")\n",
        "print(f\"üìä Methodology matches research paper: perturbation at beat {config['perturbation_beat']} with melody transposition\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Log individual model results to WandB\n",
        "print(\"=== Logging Results to WandB ===\")\n",
        "\n",
        "# Log online model results\n",
        "print(\"üìä Logging online model results...\")\n",
        "log_temporal_metrics(online_results, \"online\", config['run_name'], config['wandb_project'])\n",
        "\n",
        "# Log offline model results\n",
        "print(\"üìä Logging offline model results...\")\n",
        "log_temporal_metrics(offline_results, \"offline\", config['run_name'], config['wandb_project'])\n",
        "\n",
        "# Log baseline results\n",
        "print(\"üìä Logging baseline results...\")\n",
        "log_temporal_metrics({\"baseline\": baseline_results}, \"baseline\", config['run_name'], config['wandb_project'])\n",
        "\n",
        "print(\"‚úÖ Individual model results logged to WandB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comprehensive comparison dashboard\n",
        "print(\"\\n=== Creating WandB Comparison Dashboard ===\")\n",
        "create_wandb_comparison_dashboard(\n",
        "    online_results=online_results,\n",
        "    offline_results=offline_results,\n",
        "    baseline_results=baseline_results,\n",
        "    project_name=config['wandb_project'],\n",
        "    run_name=f\"{config['run_name']}_comparison\"\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Comprehensive comparison dashboard created!\")\n",
        "print(f\"\\nüéâ Temporal evaluation completed successfully!\")\n",
        "print(f\"üìà Check your WandB project '{config['wandb_project']}' for interactive visualizations!\")\n",
        "print(f\"üîó Look for these runs:\")\n",
        "print(f\"   ‚Ä¢ {config['run_name']}_online\")\n",
        "print(f\"   ‚Ä¢ {config['run_name']}_offline\") \n",
        "print(f\"   ‚Ä¢ {config['run_name']}_baseline\")\n",
        "print(f\"   ‚Ä¢ {config['run_name']}_comparison\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
