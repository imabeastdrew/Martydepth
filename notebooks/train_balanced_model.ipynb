{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# üéµ Balanced Chord Prediction Training\n",
        "\n",
        "This notebook implements balanced training for chord prediction that addresses class imbalance and diversity issues.\n",
        "\n",
        "## Features:\n",
        "- **üîç Data Analysis**: Automatic chord distribution analysis\n",
        "- **‚öñÔ∏è Balanced Loss Functions**: Weighted, focal, and mixed loss strategies\n",
        "- **üéØ Diversity Monitoring**: Real-time tracking of prediction diversity\n",
        "- **üé≤ Advanced Sampling**: Multiple sampling strategies with penalties\n",
        "- **üìä Comprehensive Logging**: Detailed metrics in Weights & Biases\n",
        "\n",
        "## Configuration Options:\n",
        "1. **Balanced Training**: Standard approach with weighted loss\n",
        "2. **Diversity Regularized**: Strong diversity penalties\n",
        "3. **Focal Loss Heavy**: For severe class imbalance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup: Clone repository and install package\n",
        "!git clone https://github.com/imabeastdrew/Martydepth.git\n",
        "%cd Martydepth\n",
        "\n",
        "# Install the package in development mode\n",
        "%pip install -e .\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install additional dependencies for balanced training\n",
        "%pip install torch wandb tqdm pyyaml transformers scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import AdamW\n",
        "from pathlib import Path\n",
        "import wandb\n",
        "import yaml\n",
        "import json\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "import os\n",
        "import sys\n",
        "import math\n",
        "from typing import Dict, Optional, Tuple\n",
        "\n",
        "# Add project root to Python path\n",
        "sys.path.append('.')\n",
        "\n",
        "print(\"üìç Current directory:\", os.getcwd())\n",
        "print(\"üêç Python path:\", sys.path[:3])  # Show first 3 entries\n",
        "\n",
        "# Import project modules\n",
        "from src.data.dataset import create_dataloader\n",
        "from src.models.online_transformer import OnlineTransformer\n",
        "from src.config.tokenization_config import PAD_TOKEN\n",
        "from src.training.losses import (\n",
        "    create_loss_function, analyze_prediction_diversity, MixedLossTrainer\n",
        ")\n",
        "from src.training.sampling import AdvancedSampler\n",
        "from src.data.analyze_token_distribution import analyze_chord_distribution\n",
        "from src.evaluation.training_diagnostics import quick_model_diagnostic\n",
        "\n",
        "print(\"‚úÖ All modules imported successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üéõÔ∏è Configuration Selection\n",
        "# Choose one of the three training configurations:\n",
        "\n",
        "# Option 1: Balanced Training (Recommended for first run)\n",
        "CONFIG_NAME = \"balanced_training_base\"  \n",
        "\n",
        "# Option 2: Diversity Regularized (Strong diversity penalties)\n",
        "# CONFIG_NAME = \"diversity_regularized\"\n",
        "\n",
        "# Option 3: Focal Loss Heavy (For severe class imbalance)\n",
        "# CONFIG_NAME = \"focal_loss_heavy\"\n",
        "\n",
        "# Load configuration\n",
        "config_path = f'src/training/configs/{CONFIG_NAME}.yaml'\n",
        "print(f\"üìã Loading config: {config_path}\")\n",
        "\n",
        "with open(config_path, 'r') as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'üöÄ Using device: {device}')\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"   GPU: {torch.cuda.get_device_name()}\")\n",
        "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "\n",
        "# Display key configuration settings\n",
        "print(f\"\\nüéµ Training Configuration: {CONFIG_NAME}\")\n",
        "print(f\"   Loss type: {config['loss']['type']}\")\n",
        "print(f\"   Learning rate: {config['training']['learning_rate']}\")\n",
        "print(f\"   Batch size: {config['training']['batch_size']}\")\n",
        "print(f\"   Max epochs: {config['training']['max_epochs']}\")\n",
        "print(f\"   Sequence length: {config['model']['sequence_length']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîç Data Distribution Analysis\n",
        "print(\"üîç Analyzing chord distribution in training data...\")\n",
        "\n",
        "try:\n",
        "    data_dir = Path(config['data']['data_dir'])\n",
        "    results = analyze_chord_distribution(data_dir, split=\"train\", max_files=2000)\n",
        "    \n",
        "    token_counts = results['token_counts']\n",
        "    stats = results['distribution_stats']\n",
        "    \n",
        "    print(f\"üìä Distribution Analysis Results:\")\n",
        "    print(f\"   Total unique chord tokens: {len(token_counts)}\")\n",
        "    print(f\"   Most common token: {stats['most_common_token']} ({stats['most_common_count']:,} occurrences)\")\n",
        "    print(f\"   Dominance ratio: {stats['dominance_ratio']:.3f}\")\n",
        "    print(f\"   Gini coefficient: {stats['gini_coefficient']:.3f}\")\n",
        "    print(f\"   Vocabulary coverage: {stats['vocab_coverage']:.3f}\")\n",
        "    \n",
        "    # Issue warnings based on imbalance severity\n",
        "    if stats['dominance_ratio'] > 0.5:\n",
        "        print(\"   ‚ö†Ô∏è  HIGH DOMINANCE detected - strong balancing recommended\")\n",
        "        print(\"   üí° Consider using 'focal_loss_heavy' or 'diversity_regularized' config\")\n",
        "    elif stats['dominance_ratio'] > 0.3:\n",
        "        print(\"   ‚ö†Ô∏è  MODERATE DOMINANCE detected - balanced training recommended\")\n",
        "        print(\"   üí° 'balanced_training_base' config should work well\")\n",
        "    else:\n",
        "        print(\"   ‚úÖ Reasonable distribution detected\")\n",
        "    \n",
        "    if stats['gini_coefficient'] > 0.8:\n",
        "        print(f\"   ‚ö†Ô∏è  Very high inequality (Gini: {stats['gini_coefficient']:.3f})\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"   ‚ö†Ô∏è  Could not analyze distribution: {e}\")\n",
        "    print(\"   Using standard training approach\")\n",
        "    token_counts = None\n",
        "    stats = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Weights & Biases\n",
        "run_name = f\"{CONFIG_NAME}_L{config['model']['n_layers']}_H{config['model']['n_heads']}_D{config['model']['d_model']}_seq{config['model']['sequence_length']}_bs{config['training']['batch_size']}_lr{config['training']['learning_rate']}\"\n",
        "\n",
        "wandb.init(\n",
        "    project=config['logging']['wandb']['project'],\n",
        "    name=config['logging']['wandb']['name'] + \"_\" + run_name,\n",
        "    config={\n",
        "        **config,\n",
        "        'data_distribution': stats if stats else 'unknown',\n",
        "        'num_unique_chords': len(token_counts) if token_counts else 'unknown'\n",
        "    },\n",
        "    job_type=\"balanced_training\"\n",
        ")\n",
        "\n",
        "print(f\"üìä Weights & Biases initialized: {wandb.run.name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üìä Create Data Loaders\n",
        "print(\"üìä Creating data loaders...\")\n",
        "\n",
        "data_config = config['data']\n",
        "\n",
        "train_loader, train_info = create_dataloader(\n",
        "    data_dir=Path(data_config['data_dir']),\n",
        "    split=\"train\",\n",
        "    batch_size=config['training']['batch_size'],\n",
        "    num_workers=data_config['num_workers'],\n",
        "    sequence_length=data_config['sequence_length'],\n",
        "    mode=data_config['mode'],\n",
        "    shuffle=True,\n",
        "    pin_memory=data_config.get('pin_memory', True)\n",
        ")\n",
        "\n",
        "val_loader, val_info = create_dataloader(\n",
        "    data_dir=Path(data_config['data_dir']),\n",
        "    split=\"valid\",\n",
        "    batch_size=config['training']['batch_size'],\n",
        "    num_workers=data_config['num_workers'],\n",
        "    sequence_length=data_config['sequence_length'],\n",
        "    mode=data_config['mode'],\n",
        "    shuffle=False,\n",
        "    pin_memory=data_config.get('pin_memory', True)\n",
        ")\n",
        "\n",
        "print(f\"   Train batches: {len(train_loader)}\")\n",
        "print(f\"   Valid batches: {len(val_loader)}\")\n",
        "print(f\"   Vocabulary size: {train_info.get('vocab_size', 'unknown')}\")\n",
        "\n",
        "# Update config with actual vocab size if available\n",
        "if 'vocab_size' in train_info:\n",
        "    config['model']['vocab_size'] = train_info['vocab_size']\n",
        "    print(f\"   Updated vocab_size to: {config['model']['vocab_size']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üèóÔ∏è Create Model and Loss Function\n",
        "print(\"üèóÔ∏è Creating model and loss function...\")\n",
        "\n",
        "# Initialize model\n",
        "model_config = config['model']\n",
        "model = OnlineTransformer(\n",
        "    vocab_size=model_config['vocab_size'],\n",
        "    embed_dim=model_config['d_model'],\n",
        "    num_heads=model_config['n_heads'],\n",
        "    num_layers=model_config['n_layers'],\n",
        "    max_seq_length=model_config['sequence_length'],\n",
        "    dropout=model_config['dropout'],\n",
        "    pad_token_id=PAD_TOKEN\n",
        ").to(device)\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"   Model created with {total_params:,} total parameters ({trainable_params:,} trainable)\")\n",
        "\n",
        "# Create balanced loss function\n",
        "loss_config = config['loss']\n",
        "criterion = create_loss_function(\n",
        "    loss_config=loss_config,\n",
        "    vocab_size=model_config['vocab_size'],\n",
        "    token_counts=token_counts,\n",
        "    ignore_index=loss_config.get('ignore_index', PAD_TOKEN),\n",
        "    device=device\n",
        ")\n",
        "\n",
        "print(f\"   Loss function created: {loss_config['type']}\")\n",
        "if hasattr(criterion, 'weights'):\n",
        "    print(f\"   Using class weights for {criterion.weights.sum().item():.1f} total weight\")\n",
        "\n",
        "# Create optimizer and scheduler\n",
        "train_config = config['training']\n",
        "optimizer = AdamW(\n",
        "    model.parameters(),\n",
        "    lr=train_config['learning_rate'],\n",
        "    weight_decay=train_config['weight_decay'],\n",
        "    betas=(0.9, 0.95),\n",
        "    eps=1e-8\n",
        ")\n",
        "\n",
        "# Simple scheduler for now - can enhance later\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "    optimizer, \n",
        "    T_max=train_config['max_epochs'] * len(train_loader),\n",
        "    eta_min=train_config['learning_rate'] * 0.1\n",
        ")\n",
        "\n",
        "print(f\"   Optimizer: AdamW (lr={train_config['learning_rate']}, wd={train_config['weight_decay']})\")\n",
        "print(f\"   Scheduler: CosineAnnealingLR\")\n",
        "\n",
        "# Enable gradient and parameter logging\n",
        "wandb.watch(model, log=\"all\", log_freq=config['logging']['log_every'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üéØ Training Functions with Diversity Monitoring\n",
        "def train_epoch(model, train_loader, criterion, optimizer, scheduler, device, global_step, epoch):\n",
        "    \"\"\"Training epoch with diversity monitoring.\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    diversity_history = []\n",
        "    \n",
        "    pbar = tqdm(train_loader, desc=f'Training Epoch {epoch+1}')\n",
        "    for batch_idx, batch in enumerate(pbar):\n",
        "        # Move batch to device\n",
        "        input_tokens = batch['input_tokens'].to(device)\n",
        "        target_tokens = batch['target_tokens'].to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(input_tokens)\n",
        "        \n",
        "        # Calculate loss\n",
        "        if isinstance(criterion, MixedLossTrainer):\n",
        "            loss, loss_metrics = criterion.compute_loss(logits, target_tokens, global_step)\n",
        "        else:\n",
        "            loss = criterion(logits.view(-1, logits.size(-1)), target_tokens.view(-1))\n",
        "            loss_metrics = {'loss': loss.item()}\n",
        "        \n",
        "        # Check for NaN loss\n",
        "        if torch.isnan(loss):\n",
        "            print(f\"\\n‚ö†Ô∏è NaN loss detected! Skipping batch.\")\n",
        "            continue\n",
        "        \n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), config['training']['grad_clip_norm'])\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        \n",
        "        # Get current learning rate\n",
        "        lr = optimizer.param_groups[0]['lr']\n",
        "        \n",
        "        # Analyze diversity every few steps\n",
        "        if batch_idx % 50 == 0:\n",
        "            diversity_metrics = analyze_prediction_diversity(\n",
        "                logits, target_tokens, model_config['vocab_size']\n",
        "            )\n",
        "            diversity_history.append(diversity_metrics)\n",
        "            \n",
        "            # Check for diversity alerts\n",
        "            if diversity_metrics['dominance_ratio'] > 0.7:\n",
        "                print(f\"\\n‚ö†Ô∏è HIGH DOMINANCE: {diversity_metrics['dominance_ratio']:.3f}\")\n",
        "            if diversity_metrics['vocabulary_coverage'] < 0.05:\n",
        "                print(f\"\\n‚ö†Ô∏è LOW COVERAGE: {diversity_metrics['vocabulary_coverage']:.3f}\")\n",
        "        \n",
        "        # Log metrics\n",
        "        if batch_idx % config['logging']['log_every'] == 0:\n",
        "            log_dict = {\n",
        "                'train/batch_loss': loss.item(),\n",
        "                'train/learning_rate': lr,\n",
        "                'train/grad_norm': grad_norm.item(),\n",
        "                'train/batch': batch_idx,\n",
        "                'train/epoch': epoch\n",
        "            }\n",
        "            \n",
        "            # Add loss-specific metrics\n",
        "            for key, value in loss_metrics.items():\n",
        "                log_dict[f'train/{key}'] = value\n",
        "            \n",
        "            # Add diversity metrics if available\n",
        "            if diversity_history:\n",
        "                latest_diversity = diversity_history[-1]\n",
        "                for key, value in latest_diversity.items():\n",
        "                    log_dict[f'train/diversity_{key}'] = value\n",
        "            \n",
        "            wandb.log(log_dict, step=global_step)\n",
        "        \n",
        "        global_step += 1\n",
        "        total_loss += loss.item()\n",
        "        \n",
        "        # Update progress bar\n",
        "        pbar.set_postfix({\n",
        "            'loss': f\"{loss.item():.4f}\",\n",
        "            'lr': f\"{lr:.2e}\",\n",
        "            'dom': f\"{diversity_history[-1]['dominance_ratio']:.3f}\" if diversity_history else \"---\"\n",
        "        })\n",
        "    \n",
        "    return total_loss / len(train_loader), global_step, diversity_history\n",
        "\n",
        "def validate(model, val_loader, criterion, device, vocab_size):\n",
        "    \"\"\"Validation with comprehensive diversity analysis.\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_diversity_metrics = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_loader, desc='Validating'):\n",
        "            input_tokens = batch['input_tokens'].to(device)\n",
        "            target_tokens = batch['target_tokens'].to(device)\n",
        "            \n",
        "            # Forward pass\n",
        "            logits = model(input_tokens)\n",
        "            \n",
        "            # Calculate loss\n",
        "            if isinstance(criterion, MixedLossTrainer):\n",
        "                loss, _ = criterion.compute_loss(logits, target_tokens, 0)\n",
        "            else:\n",
        "                loss = criterion(logits.view(-1, logits.size(-1)), target_tokens.view(-1))\n",
        "            \n",
        "            if not torch.isnan(loss):\n",
        "                total_loss += loss.item()\n",
        "                \n",
        "                # Analyze diversity for this batch\n",
        "                diversity_metrics = analyze_prediction_diversity(logits, target_tokens, vocab_size)\n",
        "                all_diversity_metrics.append(diversity_metrics)\n",
        "    \n",
        "    # Average validation metrics\n",
        "    avg_loss = total_loss / len(val_loader)\n",
        "    \n",
        "    # Aggregate diversity metrics\n",
        "    if all_diversity_metrics:\n",
        "        avg_diversity = {}\n",
        "        for key in all_diversity_metrics[0].keys():\n",
        "            avg_diversity[key] = np.mean([m[key] for m in all_diversity_metrics])\n",
        "    else:\n",
        "        avg_diversity = {}\n",
        "    \n",
        "    return avg_loss, avg_diversity\n",
        "\n",
        "print(\"üéØ Training functions defined with diversity monitoring\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üöÄ Main Training Loop\n",
        "print(\"üöÄ Starting balanced training...\")\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "global_step = 0\n",
        "patience_counter = 0\n",
        "max_patience = 5\n",
        "\n",
        "# Create checkpoints directory\n",
        "checkpoints_dir = Path(\"checkpoints\")\n",
        "checkpoints_dir.mkdir(exist_ok=True)\n",
        "\n",
        "try:\n",
        "    for epoch in range(config['training']['max_epochs']):\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"üéµ Epoch {epoch + 1}/{config['training']['max_epochs']}\")\n",
        "        print(f\"{'='*60}\")\n",
        "        \n",
        "        # Clear GPU memory before each epoch\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "            print(f\"üîß GPU memory at start: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
        "        \n",
        "        # Training Step\n",
        "        train_loss, global_step, train_diversity = train_epoch(\n",
        "            model, train_loader, criterion, optimizer, scheduler, device, global_step, epoch\n",
        "        )\n",
        "        \n",
        "        # Validation Step\n",
        "        val_loss, val_diversity = validate(model, val_loader, criterion, device, model_config['vocab_size'])\n",
        "        \n",
        "        # Log epoch results\n",
        "        print(f\"\\nüìä Epoch {epoch+1} Results:\")\n",
        "        print(f\"   Train Loss: {train_loss:.4f}\")\n",
        "        print(f\"   Valid Loss: {val_loss:.4f}\")\n",
        "        \n",
        "        if val_diversity:\n",
        "            print(f\"   Validation Diversity:\")\n",
        "            print(f\"     Dominance Ratio: {val_diversity['dominance_ratio']:.3f}\")\n",
        "            print(f\"     Vocab Coverage: {val_diversity['vocabulary_coverage']:.3f}\")\n",
        "            print(f\"     Prediction Entropy: {val_diversity['prediction_entropy']:.3f}\")\n",
        "            \n",
        "            # Diversity alerts\n",
        "            if val_diversity['dominance_ratio'] > 0.6:\n",
        "                print(f\"   ‚ö†Ô∏è  HIGH DOMINANCE WARNING!\")\n",
        "            if val_diversity['vocabulary_coverage'] < 0.1:\n",
        "                print(f\"   ‚ö†Ô∏è  LOW VOCABULARY COVERAGE WARNING!\")\n",
        "        \n",
        "        # Log to wandb\n",
        "        epoch_log = {\n",
        "            'train/epoch_loss': train_loss,\n",
        "            'valid/epoch_loss': val_loss,\n",
        "            'epoch': epoch + 1,\n",
        "            'train/epoch': epoch + 1\n",
        "        }\n",
        "        \n",
        "        # Add validation diversity metrics\n",
        "        for key, value in val_diversity.items():\n",
        "            epoch_log[f'valid/diversity_{key}'] = value\n",
        "        \n",
        "        wandb.log(epoch_log, step=global_step)\n",
        "        \n",
        "        # Save checkpoint if validation loss improved\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0\n",
        "            \n",
        "            # Save best checkpoint\n",
        "            checkpoint_path = checkpoints_dir / f\"balanced_best_epoch_{epoch+1}.pth\"\n",
        "            \n",
        "            checkpoint = {\n",
        "                'epoch': epoch + 1,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'scheduler_state_dict': scheduler.state_dict(),\n",
        "                'train_loss': train_loss,\n",
        "                'val_loss': val_loss,\n",
        "                'val_diversity': val_diversity,\n",
        "                'config': config,\n",
        "                'vocab_size': model_config['vocab_size'],\n",
        "                'global_step': global_step\n",
        "            }\n",
        "            \n",
        "            torch.save(checkpoint, checkpoint_path)\n",
        "            \n",
        "            # Log checkpoint as wandb artifact\n",
        "            artifact = wandb.Artifact(\n",
        "                name=f\"balanced_model_{wandb.run.id}\",\n",
        "                type=\"model\",\n",
        "                description=f\"Balanced model checkpoint from epoch {epoch+1}\"\n",
        "            )\n",
        "            artifact.add_file(str(checkpoint_path))\n",
        "            wandb.log_artifact(artifact)\n",
        "            \n",
        "            print(f\"üíæ Saved best checkpoint: {checkpoint_path}\")\n",
        "            print(f\"   New best validation loss: {val_loss:.4f}\")\n",
        "            \n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            print(f\"‚è∞ Patience counter: {patience_counter}/{max_patience}\")\n",
        "            \n",
        "            if patience_counter >= max_patience:\n",
        "                print(f\"üõë Early stopping triggered after {patience_counter} epochs without improvement\")\n",
        "                break\n",
        "        \n",
        "        # Save regular checkpoint every 5 epochs\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            regular_checkpoint_path = checkpoints_dir / f\"balanced_epoch_{epoch+1}.pth\"\n",
        "            torch.save(checkpoint, regular_checkpoint_path)\n",
        "            print(f\"üíæ Saved regular checkpoint: {regular_checkpoint_path}\")\n",
        "            \n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n‚èπÔ∏è Training interrupted by user\")\n",
        "except torch.cuda.OutOfMemoryError:\n",
        "    print(\"\\n‚ùå Out of GPU memory! Try reducing batch size or sequence length\")\n",
        "    print(f\"   Current batch size: {config['training']['batch_size']}\")\n",
        "    print(f\"   Current sequence length: {config['model']['sequence_length']}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Training failed with error: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "finally:\n",
        "    # Save final checkpoint\n",
        "    final_checkpoint_path = checkpoints_dir / \"balanced_final.pth\"\n",
        "    if 'checkpoint' in locals():\n",
        "        torch.save(checkpoint, final_checkpoint_path)\n",
        "        print(f\"üíæ Saved final checkpoint: {final_checkpoint_path}\")\n",
        "    \n",
        "    wandb.finish()\n",
        "    print(\"\\nüéâ Training completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üß™ Quick Model Evaluation\n",
        "print(\"üß™ Running quick model diagnostic...\")\n",
        "\n",
        "try:\n",
        "    # Load the best checkpoint\n",
        "    checkpoint_files = list(checkpoints_dir.glob(\"balanced_best_*.pth\"))\n",
        "    if checkpoint_files:\n",
        "        latest_checkpoint = max(checkpoint_files, key=lambda x: x.stat().st_mtime)\n",
        "        print(f\"üìÇ Loading checkpoint: {latest_checkpoint}\")\n",
        "        \n",
        "        checkpoint = torch.load(latest_checkpoint, map_location=device)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        \n",
        "        # Run diagnostic\n",
        "        results = quick_model_diagnostic(\n",
        "            model=model,\n",
        "            data_loader=val_loader,\n",
        "            device=device,\n",
        "            num_batches=10,\n",
        "            vocab_size=model_config['vocab_size']\n",
        "        )\n",
        "        \n",
        "        print(f\"\\nüìä Model Diagnostic Results:\")\n",
        "        print(f\"   Average Loss: {results['avg_loss']:.4f}\")\n",
        "        print(f\"   Prediction Accuracy: {results['accuracy']:.3f}\")\n",
        "        print(f\"   Perplexity: {results['perplexity']:.2f}\")\n",
        "        print(f\"   Unique Predictions: {results['unique_predictions']}\")\n",
        "        print(f\"   Vocabulary Coverage: {results['vocab_coverage']:.3f}\")\n",
        "        print(f\"   Dominance Ratio: {results['dominance_ratio']:.3f}\")\n",
        "        \n",
        "        # Issue alerts based on results\n",
        "        if results['dominance_ratio'] > 0.8:\n",
        "            print(\"   ‚ö†Ô∏è  CRITICAL: Very high dominance - model may be stuck\")\n",
        "        elif results['dominance_ratio'] > 0.6:\n",
        "            print(\"   ‚ö†Ô∏è  WARNING: High dominance detected\")\n",
        "        else:\n",
        "            print(\"   ‚úÖ Reasonable prediction diversity\")\n",
        "            \n",
        "        if results['vocab_coverage'] < 0.05:\n",
        "            print(\"   ‚ö†Ô∏è  CRITICAL: Very low vocabulary coverage\")\n",
        "        elif results['vocab_coverage'] < 0.1:\n",
        "            print(\"   ‚ö†Ô∏è  WARNING: Low vocabulary coverage\")\n",
        "        else:\n",
        "            print(\"   ‚úÖ Good vocabulary coverage\")\n",
        "        \n",
        "        # Log diagnostic results to wandb\n",
        "        wandb.log({\n",
        "            'diagnostic/avg_loss': results['avg_loss'],\n",
        "            'diagnostic/accuracy': results['accuracy'],\n",
        "            'diagnostic/perplexity': results['perplexity'],\n",
        "            'diagnostic/unique_predictions': results['unique_predictions'],\n",
        "            'diagnostic/vocab_coverage': results['vocab_coverage'],\n",
        "            'diagnostic/dominance_ratio': results['dominance_ratio']\n",
        "        })\n",
        "        \n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è No checkpoint found for diagnostic\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Diagnostic failed: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# üéØ Advanced Sampling Test\n",
        "\n",
        "Let's test the advanced sampling capabilities with the trained model to see if it generates diverse chord progressions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üé≤ Test Advanced Sampling\n",
        "print(\"üé≤ Testing advanced sampling strategies...\")\n",
        "\n",
        "# Create advanced sampler\n",
        "sampling_config = config.get('sampling', {})\n",
        "strategies = {\n",
        "    'nucleus': {\n",
        "        'p': sampling_config.get('top_p', 0.9),\n",
        "        'temperature': sampling_config.get('temperature', 1.2)\n",
        "    },\n",
        "    'top_k': {\n",
        "        'k': 50,\n",
        "        'temperature': sampling_config.get('temperature', 1.0)\n",
        "    }\n",
        "}\n",
        "\n",
        "sampler = AdvancedSampler(strategies=strategies)\n",
        "\n",
        "# Test with a sample from validation set\n",
        "try:\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Get a sample batch\n",
        "        sample_batch = next(iter(val_loader))\n",
        "        input_tokens = sample_batch['input_tokens'][:1].to(device)  # Take first sample\n",
        "        \n",
        "        print(f\"üéµ Input sequence length: {input_tokens.shape[1]}\")\n",
        "        print(f\"   First few tokens: {input_tokens[0, :10].cpu().tolist()}\")\n",
        "        \n",
        "        # Get model predictions\n",
        "        logits = model(input_tokens)\n",
        "        last_logits = logits[0, -1, :]  # Last position predictions\n",
        "        \n",
        "        print(f\"\\nüéØ Testing different sampling strategies:\")\n",
        "        \n",
        "        # Test multiple sampling strategies\n",
        "        strategies_to_test = ['nucleus', 'top_k']\n",
        "        \n",
        "        for strategy in strategies_to_test:\n",
        "            samples = []\n",
        "            for _ in range(10):  # Generate 10 samples\n",
        "                sample_token = sampler.sample(\n",
        "                    last_logits.unsqueeze(0),\n",
        "                    strategy=strategy,\n",
        "                    input_ids=input_tokens,\n",
        "                    repetition_penalty=1.2,\n",
        "                    frequency_penalty=0.1\n",
        "                )\n",
        "                samples.append(sample_token.item())\n",
        "            \n",
        "            unique_samples = len(set(samples))\n",
        "            print(f\"   {strategy.capitalize()}: {unique_samples}/10 unique samples\")\n",
        "            print(f\"     Samples: {samples[:5]}... (showing first 5)\")\n",
        "            \n",
        "            if unique_samples < 3:\n",
        "                print(f\"     ‚ö†Ô∏è Low diversity with {strategy}\")\n",
        "            else:\n",
        "                print(f\"     ‚úÖ Good diversity with {strategy}\")\n",
        "        \n",
        "        # Test greedy vs sampling\n",
        "        greedy_token = torch.argmax(last_logits).item()\n",
        "        nucleus_token = sampler.sample(last_logits.unsqueeze(0), 'nucleus').item()\n",
        "        \n",
        "        print(f\"\\nüîÑ Comparison:\")\n",
        "        print(f\"   Greedy prediction: {greedy_token}\")\n",
        "        print(f\"   Nucleus sample: {nucleus_token}\")\n",
        "        print(f\"   Different from greedy: {'‚úÖ' if nucleus_token != greedy_token else '‚ùå'}\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Sampling test failed: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# üìã Summary and Next Steps\n",
        "\n",
        "## Training Complete! üéâ\n",
        "\n",
        "This notebook has implemented and tested a comprehensive balanced training system for chord prediction. Here's what we accomplished:\n",
        "\n",
        "### ‚úÖ **Completed Features:**\n",
        "\n",
        "1. **üîç Data Analysis**: Automatic detection of chord distribution imbalance\n",
        "2. **‚öñÔ∏è Balanced Loss Functions**: Weighted, focal, and mixed loss strategies\n",
        "3. **üéØ Real-time Monitoring**: Diversity tracking with automatic alerts\n",
        "4. **üé≤ Advanced Sampling**: Multiple strategies with repetition penalties\n",
        "5. **üìä Comprehensive Logging**: Detailed metrics in Weights & Biases\n",
        "6. **üß™ Model Diagnostics**: Quick evaluation tools\n",
        "\n",
        "### üéõÔ∏è **Configuration Options:**\n",
        "\n",
        "- **Balanced Training**: Standard approach with weighted loss\n",
        "- **Diversity Regularized**: Strong diversity penalties  \n",
        "- **Focal Loss Heavy**: For severe class imbalance\n",
        "\n",
        "### üìà **Key Metrics to Monitor:**\n",
        "\n",
        "- **Dominance Ratio**: Should be < 0.6 for good diversity\n",
        "- **Vocabulary Coverage**: Should be > 0.1 for adequate range\n",
        "- **Prediction Entropy**: Higher values indicate more diversity\n",
        "\n",
        "### üöÄ **Next Steps:**\n",
        "\n",
        "1. **Experiment with Configurations**: Try different training configs based on your data distribution\n",
        "2. **Ablation Studies**: Compare balanced vs standard training\n",
        "3. **Generation Testing**: Use the trained model for chord progression generation\n",
        "4. **Fine-tuning**: Adjust hyperparameters based on validation metrics\n",
        "\n",
        "### üí° **Tips for Success:**\n",
        "\n",
        "- Monitor diversity metrics closely during training\n",
        "- Use early stopping if dominance ratio gets too high\n",
        "- Try different sampling strategies for generation\n",
        "- Experiment with different loss combinations\n",
        "\n",
        "Happy training! üéµ\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
