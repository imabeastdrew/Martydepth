{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "!git clone -b <midi-tokenization> --single-branch https://github.com/imabeastdrew/Martydepth.git\n",
        "%cd Martydepth\n",
        "\n",
        "# Install the package in development mode\n",
        "%pip install -e .\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "%pip install torch wandb tqdm pyyaml\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "from pathlib import Path\n",
        "import wandb\n",
        "import yaml\n",
        "import json\n",
        "from tqdm.notebook import tqdm\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Add project root to Python path\n",
        "import sys\n",
        "sys.path.append('.')\n",
        "\n",
        "print(os.getcwd())\n",
        "\n",
        "# Import project modules\n",
        "from src.data.dataset import create_dataloader\n",
        "from src.models.online_transformer import OnlineTransformer\n",
        "from src.config.tokenization_config import PAD_TOKEN\n",
        "from src.training.utils.schedulers import get_warmup_schedule\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load configuration\n",
        "print(os.getcwd())\n",
        "config_path = 'src/training/configs/online_transformer_base.yaml'\n",
        "with open(config_path, 'r') as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "# Initialize wandb\n",
        "run_name = f\"online_L{config['num_layers']}_H{config['num_heads']}_D{config['embed_dim']}_seq{config['max_sequence_length']}_bs{config['batch_size']}_lr{config['learning_rate']}\"\n",
        "\n",
        "wandb.init(\n",
        "    project=config['wandb_project'],\n",
        "    name=run_name,\n",
        "    config=config,\n",
        "    job_type=\"online_training\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Create dataloaders\n",
        "train_loader, tokenizer_info = create_dataloader(\n",
        "    data_dir=Path(config['data_dir']),\n",
        "    split=\"train\",\n",
        "    batch_size=config['batch_size'],\n",
        "    num_workers=config['num_workers'],\n",
        "    sequence_length=config['max_sequence_length'],\n",
        "    mode='online',\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_loader, _ = create_dataloader(\n",
        "    data_dir=Path(config['data_dir']),\n",
        "    split=\"valid\",\n",
        "    batch_size=config['batch_size'],\n",
        "    num_workers=config['num_workers'],\n",
        "    sequence_length=config['max_sequence_length'],\n",
        "    mode='online',\n",
        "    shuffle=False\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize model, optimizer, criterion, and scheduler\n",
        "model = OnlineTransformer(\n",
        "    vocab_size=tokenizer_info['total_vocab_size'],\n",
        "    embed_dim=config['embed_dim'],\n",
        "    num_heads=config['num_heads'],\n",
        "    num_layers=config['num_layers'],\n",
        "    dropout=config['dropout'],\n",
        "    max_seq_length=config['max_sequence_length'] * 2,  # Double for interleaved sequences\n",
        "    pad_token_id=PAD_TOKEN\n",
        ").to(device)\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=config['learning_rate'])\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)\n",
        "\n",
        "# Initialize warmup scheduler\n",
        "scheduler = get_warmup_schedule(optimizer, num_warmup_steps=config['warmup_steps'])\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "def train_epoch(model, train_loader, criterion, optimizer, scheduler, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    \n",
        "    pbar = tqdm(train_loader, desc='Training')\n",
        "    for batch_idx, batch in enumerate(pbar):\n",
        "        # Move batch to device\n",
        "        input_tokens = batch['input_tokens'].to(device)\n",
        "        target_tokens = batch['target_tokens'].to(device)\n",
        "        padding_mask = batch['padding_mask'].to(device)  # Use the padding mask from the dataloader\n",
        "        \n",
        "        # Print shapes and values for first batch\n",
        "        if batch_idx == 0:\n",
        "            print(\"\\nFirst batch debug info:\")\n",
        "            print(f\"Input tokens shape: {input_tokens.shape}\")\n",
        "            print(f\"Target tokens shape: {target_tokens.shape}\")\n",
        "            print(f\"Padding mask shape: {padding_mask.shape}\")\n",
        "            print(f\"Input tokens range: [{input_tokens.min()}, {input_tokens.max()}]\")\n",
        "            print(f\"Target tokens range: [{target_tokens.min()}, {target_tokens.max()}]\")\n",
        "            print(f\"Padding mask sum: {padding_mask.sum()}\")\n",
        "            \n",
        "            # Print first sequence\n",
        "            print(\"\\nFirst sequence in batch:\")\n",
        "            print(f\"Input:  {input_tokens[0][:20]}\")\n",
        "            print(f\"Target: {target_tokens[0][:20]}\")\n",
        "            print(f\"Mask:   {padding_mask[0][:20]}\")\n",
        "        \n",
        "        # Forward pass (causal mask is created and handled internally)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(input_tokens, padding_mask=padding_mask)\n",
        "        \n",
        "        if batch_idx == 0:\n",
        "            print(f\"\\nLogits shape: {logits.shape}\")\n",
        "            print(f\"Logits range: [{logits.min().item():.2f}, {logits.max().item():.2f}]\")\n",
        "        \n",
        "        # Calculate loss\n",
        "        loss = criterion(logits.view(-1, logits.size(-1)), target_tokens.view(-1))\n",
        "        \n",
        "        # Check for NaN loss\n",
        "        if torch.isnan(loss):\n",
        "            print(f\"\\nNaN loss detected! Skipping batch.\")\n",
        "            if epoch < 3:  # Early epochs\n",
        "                print(\"NaN in early epoch - may need to reduce learning rate or increase warmup steps\")\n",
        "            continue\n",
        "        \n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), config['gradient_clip_val'])\n",
        "        optimizer.step()\n",
        "        scheduler.step()  # Update learning rate each batch\n",
        "        \n",
        "        # Get current learning rate\n",
        "        lr = optimizer.param_groups[0]['lr']\n",
        "        \n",
        "        # Update progress bar\n",
        "        total_loss += loss.item()\n",
        "        pbar.set_postfix({'loss': loss.item(), 'lr': f\"{lr:.2e}\"})\n",
        "        \n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "def validate(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    nan_batches = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_loader, desc='Validating'):\n",
        "            # Move batch to device\n",
        "            input_tokens = batch['input_tokens'].to(device)\n",
        "            target_tokens = batch['target_tokens'].to(device)\n",
        "            padding_mask = batch['padding_mask'].to(device)  # Use the padding mask from the dataloader\n",
        "            \n",
        "            # Forward pass (causal mask is created and handled internally)\n",
        "            logits = model(input_tokens, padding_mask=padding_mask)\n",
        "            \n",
        "            # Calculate loss\n",
        "            loss = criterion(logits.view(-1, logits.size(-1)), target_tokens.view(-1))\n",
        "            \n",
        "            # Check for NaN loss\n",
        "            if torch.isnan(loss):\n",
        "                nan_batches += 1\n",
        "                continue\n",
        "                \n",
        "            total_loss += loss.item()\n",
        "    \n",
        "    # Avoid division by zero if all batches were NaN\n",
        "    num_valid_batches = len(val_loader) - nan_batches\n",
        "    return total_loss / num_valid_batches if num_valid_batches > 0 else float('nan')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training loop\n",
        "best_val_loss = float('inf')\n",
        "global_step = 0\n",
        "\n",
        "try:\n",
        "    for epoch in range(config['max_epochs']):\n",
        "        print(f\"\\nEpoch {epoch + 1}/{config['max_epochs']}\")\n",
        "        \n",
        "        # Clear GPU memory before each epoch\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "            print(f\"GPU memory at start of epoch: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
        "        \n",
        "        # Training Step\n",
        "        train_loss = train_epoch(model, train_loader, criterion, optimizer, scheduler, device)\n",
        "        \n",
        "        # Validation Step\n",
        "        val_loss = validate(model, val_loader, criterion, device)\n",
        "        \n",
        "        print(f\"Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Valid Loss: {val_loss:.4f}\")\n",
        "        wandb.log({\n",
        "            'train/epoch_loss': train_loss,\n",
        "            'valid/epoch_loss': val_loss,\n",
        "            'epoch': epoch + 1\n",
        "        }, step=global_step)\n",
        "        \n",
        "        # Save checkpoint if validation loss improved\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            \n",
        "            # Save checkpoint locally\n",
        "            checkpoint_path = Path(\"checkpoints\") / f\"online_transformer_epoch_{epoch+1}.pth\"\n",
        "            checkpoint_path.parent.mkdir(exist_ok=True)\n",
        "            \n",
        "            checkpoint = {\n",
        "                'epoch': epoch + 1,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'scheduler_state_dict': scheduler.state_dict(),\n",
        "                'train_loss': train_loss,\n",
        "                'val_loss': val_loss,\n",
        "                'config': config,\n",
        "            }\n",
        "            \n",
        "            torch.save(checkpoint, checkpoint_path)\n",
        "            \n",
        "            # Log checkpoint as wandb artifact\n",
        "            artifact = wandb.Artifact(\n",
        "                name=f\"online_transformer_model_{wandb.run.id}\",\n",
        "                type=\"model\",\n",
        "                description=f\"Online Transformer Model checkpoint from epoch {epoch+1}\"\n",
        "            )\n",
        "            artifact.add_file(str(checkpoint_path))\n",
        "            wandb.log_artifact(artifact)\n",
        "            \n",
        "            # Also save tokenizer info as artifact\n",
        "            tokenizer_artifact = wandb.Artifact(\n",
        "                name=f\"tokenizer_info_{wandb.run.id}\",\n",
        "                type=\"tokenizer\",\n",
        "                description=\"Tokenizer information used for training\"\n",
        "            )\n",
        "            tokenizer_path = Path(\"tokenizer_info.json\")\n",
        "            with open(tokenizer_path, 'w') as f:\n",
        "                json.dump(tokenizer_info, f)\n",
        "            tokenizer_artifact.add_file(str(tokenizer_path))\n",
        "            wandb.log_artifact(tokenizer_artifact)\n",
        "            \n",
        "            print(f\"\\nSaved checkpoint with validation loss: {val_loss:.4f}\")\n",
        "            \n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\nTraining interrupted by user\")\n",
        "except torch.cuda.OutOfMemoryError:\n",
        "    print(\"\\nOut of GPU memory! Try reducing batch size or sequence length\")\n",
        "finally:\n",
        "    wandb.finish()\n",
        "    print(\"\\nTraining completed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_best_checkpoint(model, checkpoint_path):\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    return checkpoint['val_loss']\n",
        "\n",
        "# Example usage:\n",
        "# best_checkpoint_path = Path(wandb.run.dir) / 'best_checkpoint.pth'\n",
        "# best_val_loss = load_best_checkpoint(model, best_checkpoint_path)\n",
        "# print(f\"Loaded checkpoint with validation loss: {best_val_loss:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
