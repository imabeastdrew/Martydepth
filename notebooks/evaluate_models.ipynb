{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!git clone https://github.com/imabeastdrew/Martydepth.git\n",
        "%cd Martydepth\n",
        "\n",
        "# Install the package in development mode\n",
        "%pip install -e .\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "%pip install torch wandb tqdm pyyaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import torch\n",
        "import wandb\n",
        "import tempfile\n",
        "import json\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from tqdm.notebook import tqdm\n",
        "import os\n",
        "import sys\n",
        "import yaml\n",
        "\n",
        "# Add project root to Python path\n",
        "sys.path.append('.')\n",
        "\n",
        "print(os.getcwd())\n",
        "\n",
        "# Import project modules\n",
        "from src.data.dataset import create_dataloader\n",
        "from src.models.online_transformer import OnlineTransformer\n",
        "from src.models.offline_teacher import OfflineTeacherModel\n",
        "from src.evaluation.metrics import (\n",
        "    calculate_harmony_metrics,\n",
        "    calculate_emd_metrics,\n",
        ")\n",
        "from src.config.tokenization_config import (\n",
        "    SILENCE_TOKEN,\n",
        "    MELODY_ONSET_HOLD_START,\n",
        "    CHORD_TOKEN_START,\n",
        ")\n",
        "from src.evaluation.evaluate_offline import generate_offline\n",
        "from src.evaluation.evaluate import generate_online\n",
        "from src.config.tokenization_config import PAD_TOKEN\n",
        "\n",
        "def get_latest_model_artifact(run_id: str, model_type: str) -> tuple[Path, dict]:\n",
        "    \"\"\"\n",
        "    Retrieve the latest model artifact and its config from wandb.\n",
        "    \n",
        "    Args:\n",
        "        run_id: The wandb run ID to get artifacts from\n",
        "        model_type: Type of model ('online', 'offline', 'discriminator', or 'contrastive')\n",
        "    \n",
        "    Returns:\n",
        "        tuple[Path, dict]: Path to the checkpoint file and the model config\n",
        "    \"\"\"\n",
        "    # Map model types to their artifact names\n",
        "    artifact_prefix_map = {\n",
        "        'online': 'online_transformer_model',\n",
        "        'offline': 'offline_teacher_model',\n",
        "        'discriminator': 'discriminator_reward_model',\n",
        "        'contrastive': 'contrastive_reward_model'\n",
        "    }\n",
        "    \n",
        "    prefix = artifact_prefix_map[model_type]\n",
        "    artifact_name = f\"{prefix}_{run_id}\"\n",
        "    \n",
        "    # Get the latest version of the model artifact\n",
        "    api = wandb.Api()\n",
        "    artifact = api.artifact(f\"{wandb.run.entity}/{wandb.run.project}/{artifact_name}:latest\")\n",
        "    \n",
        "    # Download the artifact\n",
        "    artifact_dir = Path(\"artifacts\") / model_type\n",
        "    artifact_dir.mkdir(parents=True, exist_ok=True)\n",
        "    artifact.download(str(artifact_dir))\n",
        "    \n",
        "    # Find the checkpoint file\n",
        "    checkpoint_file = next(artifact_dir.glob(\"*.pth\"))\n",
        "    \n",
        "    # Load the checkpoint to get the config\n",
        "    checkpoint = torch.load(checkpoint_file, map_location='cpu')\n",
        "    config = checkpoint['config']\n",
        "    \n",
        "    return checkpoint_file, config\n",
        "\n",
        "def get_tokenizer_artifact(run_id: str) -> dict:\n",
        "    \"\"\"\n",
        "    Retrieve the tokenizer info artifact from wandb.\n",
        "    \n",
        "    Args:\n",
        "        run_id: The wandb run ID to get artifacts from\n",
        "    \n",
        "    Returns:\n",
        "        dict: The tokenizer info dictionary\n",
        "    \"\"\"\n",
        "    # Get the latest version of the tokenizer artifact\n",
        "    api = wandb.Api()\n",
        "    artifact = api.artifact(f\"{wandb.run.entity}/{wandb.run.project}/tokenizer_info_{run_id}:latest\")\n",
        "    \n",
        "    # Download the artifact\n",
        "    artifact_dir = Path(\"artifacts\") / \"tokenizer\"\n",
        "    artifact_dir.mkdir(parents=True, exist_ok=True)\n",
        "    artifact.download(str(artifact_dir))\n",
        "    \n",
        "    # Load the tokenizer info\n",
        "    tokenizer_file = next(artifact_dir.glob(\"*.json\"))\n",
        "    with open(tokenizer_file, 'r') as f:\n",
        "        tokenizer_info = json.load(f)\n",
        "    \n",
        "    return tokenizer_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "config = {\n",
        "    'data_dir': 'data/interim',\n",
        "    'split': 'test',\n",
        "    'batch_size': 32,\n",
        "    'num_workers': 4,\n",
        "    'temperature': 1.0,\n",
        "    'top_k': 50,\n",
        "    # Specify the run IDs to evaluate\n",
        "    'online_run_id': None,  # Set this to the wandb run ID of the online model\n",
        "    'offline_run_id': None, # Set this to the wandb run ID of the offline model\n",
        "}\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else\n",
        "                     \"mps\" if torch.backends.mps.is_available() else\n",
        "                     \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Initialize wandb\n",
        "wandb.init(\n",
        "    project=\"martydepth\",\n",
        "    name=\"model_evaluation\",\n",
        "    config=config,\n",
        "    job_type=\"evaluation\"\n",
        ")\n",
        "\n",
        "# Get model artifacts and configs\n",
        "if config['online_run_id']:\n",
        "    online_checkpoint_path, online_config = get_latest_model_artifact(config['online_run_id'], 'online')\n",
        "    print(f\"Retrieved online model checkpoint from wandb: {online_checkpoint_path}\")\n",
        "else:\n",
        "    print(\"No online run ID provided, skipping online model evaluation\")\n",
        "    online_checkpoint_path, online_config = None, None\n",
        "\n",
        "if config['offline_run_id']:\n",
        "    offline_checkpoint_path, offline_config = get_latest_model_artifact(config['offline_run_id'], 'offline')\n",
        "    print(f\"Retrieved offline model checkpoint from wandb: {offline_checkpoint_path}\")\n",
        "else:\n",
        "    print(\"No offline run ID provided, skipping offline model evaluation\")\n",
        "    offline_checkpoint_path, offline_config = None, None\n",
        "\n",
        "# Get tokenizer info from either run (they should be the same)\n",
        "run_id_for_tokenizer = config['online_run_id'] or config['offline_run_id']\n",
        "if run_id_for_tokenizer:\n",
        "    tokenizer_info = get_tokenizer_artifact(run_id_for_tokenizer)\n",
        "    print(\"Retrieved tokenizer info from wandb\")\n",
        "else:\n",
        "    # Fallback to local tokenizer info if no run IDs provided\n",
        "    tokenizer_info_path = Path(config['data_dir']) / \"train\" / \"tokenizer_info.json\"\n",
        "    with open(tokenizer_info_path, 'r') as f:\n",
        "        tokenizer_info = json.load(f)\n",
        "    print(\"Using local tokenizer info\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate Online Model\n",
        "if online_checkpoint_path and online_config:\n",
        "    print(\"\\n=== Evaluating Online Model ===\")\n",
        "    print(f\"Loading model from checkpoint: {online_checkpoint_path}\")\n",
        "\n",
        "    # Initialize model\n",
        "    model = OnlineTransformer(\n",
        "        vocab_size=tokenizer_info['total_vocab_size'],\n",
        "        embed_dim=online_config['embed_dim'],\n",
        "        num_heads=online_config['num_heads'],\n",
        "        num_layers=online_config['num_layers'],\n",
        "        dropout=online_config['dropout'],\n",
        "        max_seq_length=online_config.get('max_seq_length', 512),\n",
        "        pad_token_id=PAD_TOKEN # Use PAD_TOKEN here\n",
        "    ).to(device)\n",
        "\n",
        "    # Load checkpoint\n",
        "    checkpoint = torch.load(online_checkpoint_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.eval()\n",
        "\n",
        "    # Create dataloader\n",
        "    max_seq_length = online_config.get('max_seq_length') or online_config.get('max_sequence_length') or 512\n",
        "    dataloader, _ = create_dataloader(\n",
        "        data_dir=Path(config['data_dir']),\n",
        "        split=config['split'],\n",
        "        batch_size=config['batch_size'],\n",
        "        num_workers=config['num_workers'],\n",
        "        sequence_length=max_seq_length,\n",
        "        mode='online',\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    # Generate sequences\n",
        "    print(\"\\nGenerating sequences...\")\n",
        "    generated_sequences, ground_truth_sequences = generate_online(\n",
        "        model=model,\n",
        "        dataloader=dataloader,\n",
        "        tokenizer_info=tokenizer_info,\n",
        "        device=device,\n",
        "        temperature=config['temperature'],\n",
        "        top_k=config['top_k']\n",
        "    )\n",
        "\n",
        "    # Calculate metrics\n",
        "    print(\"\\nCalculating metrics...\")\n",
        "    harmony_metrics = calculate_harmony_metrics(generated_sequences, tokenizer_info)\n",
        "    emd_metrics = calculate_emd_metrics(generated_sequences, ground_truth_sequences, tokenizer_info)\n",
        "\n",
        "    online_metrics = {**harmony_metrics, **emd_metrics}\n",
        "\n",
        "    print(\"\\n=== Online Model Results ===\")\n",
        "    for metric, value in online_metrics.items():\n",
        "        print(f\"{metric}: {value:.4f}\")\n",
        "        wandb.log({f\"online/{metric}\": value})\n",
        "        \n",
        "    # Log the run ID we evaluated\n",
        "    wandb.log({\"online_model_run_id\": config['online_run_id']})\n",
        "else:\n",
        "    print(\"\\n=== Skipping Online Model Evaluation ===\")\n",
        "    print(\"No online model checkpoint provided\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate Offline Model\n",
        "if offline_checkpoint_path and offline_config:\n",
        "    print(\"\\n=== Evaluating Offline Model ===\")\n",
        "    print(f\"Loading model from checkpoint: {offline_checkpoint_path}\")\n",
        "\n",
        "    # Initialize model\n",
        "    model = OfflineTeacherModel(\n",
        "        melody_vocab_size=tokenizer_info['melody_vocab_size'],\n",
        "        chord_vocab_size=tokenizer_info['chord_vocab_size'],\n",
        "        embed_dim=offline_config['embed_dim'],\n",
        "        num_heads=offline_config['num_heads'],\n",
        "        num_layers=offline_config['num_layers'],\n",
        "        max_seq_length=offline_config.get('max_seq_length', 512),\n",
        "        pad_token_id=PAD_TOKEN # Use PAD_TOKEN here\n",
        "    ).to(device)\n",
        "\n",
        "    # Load checkpoint\n",
        "    checkpoint = torch.load(offline_checkpoint_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.eval()\n",
        "\n",
        "    # Create dataloader\n",
        "    max_seq_length = offline_config.get('max_seq_length') or offline_config.get('max_sequence_length') or 512\n",
        "    dataloader, _ = create_dataloader(\n",
        "        data_dir=Path(config['data_dir']),\n",
        "        split=config['split'],\n",
        "        batch_size=config['batch_size'],\n",
        "        num_workers=config['num_workers'],\n",
        "        sequence_length=max_seq_length,\n",
        "        mode='offline',\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    # Generate sequences\n",
        "    print(\"\\nGenerating sequences...\")\n",
        "    generated_sequences, ground_truth_sequences = generate_offline(\n",
        "        model=model,\n",
        "        dataloader=dataloader,\n",
        "        tokenizer_info=tokenizer_info,\n",
        "        device=device,\n",
        "        temperature=config['temperature'],\n",
        "        top_k=config['top_k']\n",
        "    )\n",
        "\n",
        "    # Calculate metrics\n",
        "    print(\"\\nCalculating metrics...\")\n",
        "    harmony_metrics = calculate_harmony_metrics(generated_sequences, tokenizer_info)\n",
        "    emd_metrics = calculate_emd_metrics(generated_sequences, ground_truth_sequences, tokenizer_info)\n",
        "\n",
        "    offline_metrics = {**harmony_metrics, **emd_metrics}\n",
        "\n",
        "    print(\"\\n=== Offline Model Results ===\")\n",
        "    for metric, value in offline_metrics.items():\n",
        "        print(f\"{metric}: {value:.4f}\")\n",
        "        wandb.log({f\"offline/{metric}\": value})\n",
        "        \n",
        "    # Log the run ID we evaluated\n",
        "    wandb.log({\"offline_model_run_id\": config['offline_run_id']})\n",
        "else:\n",
        "    print(\"\\n=== Skipping Offline Model Evaluation ===\")\n",
        "    print(\"No offline model checkpoint provided\")\n",
        "generated_sequences, ground_truth_sequences = generate_offline(\n",
        "    model=model,\n",
        "    dataloader=dataloader,\n",
        "    tokenizer_info=tokenizer_info,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "# Calculate metrics\n",
        "print(\"\\nCalculating metrics...\")\n",
        "harmony_metrics = calculate_harmony_metrics(generated_sequences, tokenizer_info)\n",
        "emd_metrics = calculate_emd_metrics(generated_sequences, ground_truth_sequences, tokenizer_info)\n",
        "\n",
        "offline_metrics = {**harmony_metrics, **emd_metrics}\n",
        "\n",
        "print(\"\\n=== Offline Model Results ===\")\n",
        "for metric, value in offline_metrics.items():\n",
        "    print(f\"{metric}: {value:.4f}\")\n",
        "    wandb.log({f\"offline/{metric}\": value})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare Results\n",
        "print(\"\\n=== Model Comparison ===\")\n",
        "print(f\"{'Metric':<30} {'Online':<10} {'Offline':<10} {'Difference':<10}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for metric in online_metrics.keys():\n",
        "    online_value = online_metrics[metric]\n",
        "    offline_value = offline_metrics[metric]\n",
        "    diff = online_value - offline_value\n",
        "    print(f\"{metric:<30} {online_value:>10.4f} {offline_value:>10.4f} {diff:>10.4f}\")\n",
        "    wandb.log({\n",
        "        f\"comparison/{metric}_diff\": diff,\n",
        "        f\"comparison/{metric}_ratio\": online_value / offline_value if offline_value != 0 else 0\n",
        "    })\n",
        "\n",
        "# Finish wandb run\n",
        "wandb.finish()"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
