{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!git clone https://github.com/imabeastdrew/Martydepth.git\n",
        "%cd Martydepth\n",
        "\n",
        "# Install the package in development mode\n",
        "%pip install -e .\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "%pip install torch wandb tqdm pyyaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import torch\n",
        "import wandb\n",
        "import tempfile\n",
        "import json\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from tqdm.notebook import tqdm\n",
        "import os\n",
        "import sys\n",
        "import yaml\n",
        "\n",
        "# Add project root to Python path\n",
        "sys.path.append('.')\n",
        "\n",
        "print(os.getcwd())\n",
        "\n",
        "# Import project modules\n",
        "from src.data.dataset import create_dataloader\n",
        "from src.models.online_transformer import OnlineTransformer\n",
        "from src.models.offline_teacher import OfflineTeacherModel\n",
        "from src.evaluation.metrics import (\n",
        "    calculate_harmony_metrics,\n",
        "    calculate_emd_metrics,\n",
        ")\n",
        "from src.config.tokenization_config import (\n",
        "    SILENCE_TOKEN,\n",
        "    MELODY_ONSET_HOLD_START,\n",
        "    CHORD_TOKEN_START,\n",
        ")\n",
        "from src.evaluation.evaluate_offline import generate_offline\n",
        "from src.evaluation.evaluate import generate_online\n",
        "from src.config.tokenization_config import PAD_TOKEN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "config = {\n",
        "    'data_dir': 'data/interim',\n",
        "    'split': 'test',\n",
        "    'batch_size': 32,\n",
        "    'num_workers': 4,\n",
        "    'temperature': 1.0,\n",
        "    'top_k': 50,\n",
        "}\n",
        "\n",
        "# Get checkpoint paths from user input with default paths from Google Drive\n",
        "online_checkpoint_path = \"/content/drive/MyDrive/Martydepth/online_transformer_epoch_11.pth\"\n",
        "online_config_path = \"/content/Martydepth/src/training/configs/online_transformer_base.yaml\"\n",
        "\n",
        "offline_checkpoint_path = \"/content/drive/MyDrive/Martydepth/offline_teacher_epoch_12.pth\"\n",
        "offline_config_path = \"/content/Martydepth/src/training/configs/offline_teacher_base.yaml\"\n",
        "\n",
        "\n",
        "# Load tokenizer info\n",
        "tokenizer_info_path = Path(config['data_dir']) / \"train\" / \"tokenizer_info.json\"\n",
        "with open(tokenizer_info_path, 'r') as f:\n",
        "    tokenizer_info = json.load(f)\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else\n",
        "                     \"mps\" if torch.backends.mps.is_available() else\n",
        "                     \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Initialize wandb\n",
        "wandb.init(\n",
        "    project=\"martydepth\",\n",
        "    name=\"model_evaluation\",\n",
        "    config=config,\n",
        "    job_type=\"evaluation\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate Online Model\n",
        "print(\"\\n=== Evaluating Online Model ===\")\n",
        "print(f\"Loading model from checkpoint: {online_checkpoint_path}\")\n",
        "\n",
        "# Load model config\n",
        "with open(online_config_path, 'r') as f:\n",
        "    online_config = yaml.safe_load(f)\n",
        "\n",
        "# Initialize model\n",
        "model = OnlineTransformer(\n",
        "    vocab_size=tokenizer_info['total_vocab_size'],\n",
        "    embed_dim=online_config['embed_dim'],\n",
        "    num_heads=online_config['num_heads'],\n",
        "    num_layers=online_config['num_layers'],\n",
        "    dropout=online_config['dropout'],\n",
        "    max_seq_length=online_config.get('max_seq_length', 512),\n",
        "    pad_token_id=PAD_TOKEN # Use PAD_TOKEN here\n",
        ").to(device)\n",
        "\n",
        "# Load checkpoint\n",
        "checkpoint = torch.load(online_checkpoint_path, map_location=device)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()\n",
        "\n",
        "# Create dataloader\n",
        "max_seq_length = online_config.get('max_seq_length') or online_config.get('max_sequence_length') or 512\n",
        "dataloader, _ = create_dataloader(\n",
        "    data_dir=Path(config['data_dir']),\n",
        "    split=config['split'],\n",
        "    batch_size=config['batch_size'],\n",
        "    num_workers=config['num_workers'],\n",
        "    sequence_length=max_seq_length,\n",
        "    mode='online',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Generate sequences\n",
        "print(\"\\nGenerating sequences...\")\n",
        "generated_sequences, ground_truth_sequences = generate_online(\n",
        "    model=model,\n",
        "    dataloader=dataloader,\n",
        "    tokenizer_info=tokenizer_info,\n",
        "    device=device,\n",
        "    temperature=config['temperature'],\n",
        "    top_k=config['top_k']\n",
        ")\n",
        "\n",
        "# Calculate metrics\n",
        "print(\"\\nCalculating metrics...\")\n",
        "harmony_metrics = calculate_harmony_metrics(generated_sequences, tokenizer_info)\n",
        "emd_metrics = calculate_emd_metrics(generated_sequences, ground_truth_sequences, tokenizer_info)\n",
        "\n",
        "online_metrics = {**harmony_metrics, **emd_metrics}\n",
        "\n",
        "print(\"\\n=== Online Model Results ===\")\n",
        "for metric, value in online_metrics.items():\n",
        "    print(f\"{metric}: {value:.4f}\")\n",
        "    wandb.log({f\"online/{metric}\": value})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate Offline Model\n",
        "print(\"\\n=== Evaluating Offline Model ===\")\n",
        "print(f\"Loading model from checkpoint: {offline_checkpoint_path}\")\n",
        "\n",
        "# Load model config\n",
        "with open(offline_config_path, 'r') as f:\n",
        "    offline_config = yaml.safe_load(f)\n",
        "\n",
        "# Initialize model\n",
        "model = OfflineTeacherModel(\n",
        "    melody_vocab_size=tokenizer_info['melody_vocab_size'],\n",
        "    chord_vocab_size=tokenizer_info['chord_vocab_size'],\n",
        "    embed_dim=offline_config['embed_dim'],\n",
        "    num_heads=offline_config['num_heads'],\n",
        "    num_layers=offline_config['num_layers'],\n",
        "    max_seq_length=offline_config.get('max_seq_length', 512),\n",
        "    pad_token_id=PAD_TOKEN # Use PAD_TOKEN here\n",
        ").to(device)\n",
        "\n",
        "# Load checkpoint\n",
        "checkpoint = torch.load(offline_checkpoint_path, map_location=device)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()\n",
        "\n",
        "# Create dataloader\n",
        "max_seq_length = offline_config.get('max_seq_length') or offline_config.get('max_sequence_length') or 512\n",
        "dataloader, _ = create_dataloader(\n",
        "    data_dir=Path(config['data_dir']),\n",
        "    split=config['split'],\n",
        "    batch_size=config['batch_size'],\n",
        "    num_workers=config['num_workers'],\n",
        "    sequence_length=max_seq_length,\n",
        "    mode='offline',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Generate sequences\n",
        "print(\"\\nGenerating sequences...\")\n",
        "generated_sequences, ground_truth_sequences = generate_offline(\n",
        "    model=model,\n",
        "    dataloader=dataloader,\n",
        "    tokenizer_info=tokenizer_info,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "# Calculate metrics\n",
        "print(\"\\nCalculating metrics...\")\n",
        "harmony_metrics = calculate_harmony_metrics(generated_sequences, tokenizer_info)\n",
        "emd_metrics = calculate_emd_metrics(generated_sequences, ground_truth_sequences, tokenizer_info)\n",
        "\n",
        "offline_metrics = {**harmony_metrics, **emd_metrics}\n",
        "\n",
        "print(\"\\n=== Offline Model Results ===\")\n",
        "for metric, value in offline_metrics.items():\n",
        "    print(f\"{metric}: {value:.4f}\")\n",
        "    wandb.log({f\"offline/{metric}\": value})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare Results\n",
        "print(\"\\n=== Model Comparison ===\")\n",
        "print(f\"{'Metric':<30} {'Online':<10} {'Offline':<10} {'Difference':<10}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for metric in online_metrics.keys():\n",
        "    online_value = online_metrics[metric]\n",
        "    offline_value = offline_metrics[metric]\n",
        "    diff = online_value - offline_value\n",
        "    print(f\"{metric:<30} {online_value:>10.4f} {offline_value:>10.4f} {diff:>10.4f}\")\n",
        "    wandb.log({\n",
        "        f\"comparison/{metric}_diff\": diff,\n",
        "        f\"comparison/{metric}_ratio\": online_value / offline_value if offline_value != 0 else 0\n",
        "    })\n",
        "\n",
        "# Finish wandb run\n",
        "wandb.finish()"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
