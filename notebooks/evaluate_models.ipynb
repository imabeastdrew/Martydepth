{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!git clone https://github.com/imabeastdrew/Martydepth.git\n",
        "%cd Martydepth\n",
        "\n",
        "# Install the package in development mode\n",
        "%pip install -e .\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "%pip install torch wandb tqdm pyyaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import torch\n",
        "import wandb\n",
        "import json\n",
        "import numpy as np\n",
        "import tempfile\n",
        "from pathlib import Path\n",
        "from tqdm.notebook import tqdm\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Add project root to Python path\n",
        "sys.path.append('.')\n",
        "\n",
        "# Import project modules\n",
        "from src.data.dataset import create_dataloader\n",
        "from src.models.online_transformer import OnlineTransformer\n",
        "from src.models.offline_teacher_t5 import T5OfflineTeacherModel\n",
        "from src.evaluation.metrics import (\n",
        "    calculate_harmony_metrics,\n",
        "    calculate_emd_metrics,\n",
        "    parse_sequences,  # Added this import\n",
        ")\n",
        "from src.config.tokenization_config import (\n",
        "    SILENCE_TOKEN,\n",
        "    MELODY_ONSET_HOLD_START,\n",
        "    CHORD_TOKEN_START,\n",
        "    PAD_TOKEN,\n",
        ")\n",
        "from src.evaluation.evaluate_offline import generate_offline\n",
        "from src.evaluation.evaluate import generate_online\n",
        "\n",
        "def load_model_artifact(artifact_path: str) -> tuple[dict, dict, dict]:\n",
        "    \"\"\"\n",
        "    Load a model artifact and its config from wandb.\n",
        "    Compatible with both checkpoint-style and separate file artifacts.\n",
        "    \n",
        "    Args:\n",
        "        artifact_path: Full path to the artifact (e.g. 'marty1ai/martydepth/model_name:version')\n",
        "    \n",
        "    Returns:\n",
        "        tuple[dict, dict, dict]: Model state dict, config, and tokenizer_info\n",
        "    \"\"\"\n",
        "    api = wandb.Api()\n",
        "    artifact = api.artifact(artifact_path)\n",
        "    \n",
        "    # Download the artifact\n",
        "    with tempfile.TemporaryDirectory() as tmp_dir:\n",
        "        artifact_dir = artifact.download(tmp_dir)\n",
        "        artifact_path_obj = Path(artifact_dir)\n",
        "        \n",
        "        # Check for different artifact structures\n",
        "        checkpoint_files = list(artifact_path_obj.glob(\"*.pth\"))\n",
        "        tokenizer_files = list(artifact_path_obj.glob(\"tokenizer_info.json\"))\n",
        "        \n",
        "        if len(checkpoint_files) == 1 and len(tokenizer_files) == 1:\n",
        "            # Separate files structure (model.pth + tokenizer_info.json)\n",
        "            model_path = checkpoint_files[0]\n",
        "            tokenizer_path = tokenizer_files[0]\n",
        "            \n",
        "            # Load model state dict directly\n",
        "            model_state_dict = torch.load(model_path, map_location='cpu', weights_only=True)\n",
        "            \n",
        "            # Load tokenizer info\n",
        "            with open(tokenizer_path, 'r') as f:\n",
        "                tokenizer_info = json.load(f)\n",
        "            \n",
        "            # Get config from the run that created this artifact\n",
        "            run = artifact.logged_by()\n",
        "            config = dict(run.config)\n",
        "            \n",
        "        elif len(checkpoint_files) == 1 and len(tokenizer_files) == 0:\n",
        "            # Checkpoint structure (single .pth file with everything)\n",
        "            checkpoint_file = checkpoint_files[0]\n",
        "            checkpoint = torch.load(checkpoint_file, map_location='cpu')\n",
        "            \n",
        "            if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:\n",
        "                # Training checkpoint structure\n",
        "                model_state_dict = checkpoint['model_state_dict']\n",
        "                config = checkpoint.get('config', {})\n",
        "                \n",
        "                # Try to get tokenizer info from run config or load from data dir\n",
        "                run = artifact.logged_by()\n",
        "                run_config = dict(run.config)\n",
        "                config.update(run_config)  # Merge configs\n",
        "                \n",
        "                # Load tokenizer info from data directory as fallback\n",
        "                tokenizer_info_path = Path(\"data/interim/train/tokenizer_info.json\")\n",
        "                if tokenizer_info_path.exists():\n",
        "                    with open(tokenizer_info_path, 'r') as f:\n",
        "                        tokenizer_info = json.load(f)\n",
        "                else:\n",
        "                    raise FileNotFoundError(\"Could not find tokenizer_info.json in checkpoint or data directory\")\n",
        "            else:\n",
        "                # Direct state dict\n",
        "                model_state_dict = checkpoint\n",
        "                run = artifact.logged_by()\n",
        "                config = dict(run.config)\n",
        "                \n",
        "                # Load tokenizer info from data directory\n",
        "                tokenizer_info_path = Path(\"data/interim/train/tokenizer_info.json\")\n",
        "                with open(tokenizer_info_path, 'r') as f:\n",
        "                    tokenizer_info = json.load(f)\n",
        "        else:\n",
        "            raise ValueError(f\"Unexpected artifact structure in {artifact_dir}. Expected either model.pth+tokenizer_info.json or single checkpoint.pth\")\n",
        "    \n",
        "    return model_state_dict, config, tokenizer_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "config = {\n",
        "    # Data parameters\n",
        "    'data_dir': 'data/interim',\n",
        "    'split': 'test',\n",
        "    'batch_size': 32,\n",
        "    'num_workers': 4,\n",
        "    \n",
        "    # Sampling parameters\n",
        "    'temperature': 1.0,     # Increased from 1.3 for more exploration\n",
        "    'top_k': 50,           # Reduced from 30 to focus on more likely but still diverse options\n",
        "    'wait_beats': 1,       # double\n",
        "    \n",
        "    # Model artifact paths\n",
        "    'online_model_artifact': 'marty1ai/martydepth/online_transformer_model_bvwago40:v13',\n",
        "    'offline_model_artifact': 'marty1ai/martydepth/offline_teacher_model_2hd3b6gi:v8'\n",
        "}\n",
        "\n",
        "# Add some helpful frame/beat conversions\n",
        "frames_per_beat = 4  # Standard in our dataset\n",
        "\n",
        "print(f\"\\nSampling Parameters:\")\n",
        "print(f\"Temperature: {config['temperature']}\")\n",
        "print(f\"Top-k: {config['top_k']}\")\n",
        "print(f\"Wait beats: {config['wait_beats']}\")\n",
        "\n",
        "# Import necessary constants\n",
        "from src.config.tokenization_config import (\n",
        "    MELODY_VOCAB_SIZE,\n",
        "    CHORD_TOKEN_START,\n",
        "    SILENCE_TOKEN,\n",
        "    PAD_TOKEN\n",
        ")\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else\n",
        "                     \"mps\" if torch.backends.mps.is_available() else\n",
        "                     \"cpu\")\n",
        "print(f\"\\nUsing device: {device}\")\n",
        "\n",
        "# Initialize wandb\n",
        "wandb.init(\n",
        "    project=\"martydepth\",\n",
        "    name=\"model_evaluation_configurable_durations\",  # Updated name to reflect changes\n",
        "    config=config,\n",
        "    job_type=\"evaluation\"\n",
        ")\n",
        "\n",
        "# Load model artifacts\n",
        "print(\"\\nLoading model artifacts...\")\n",
        "\n",
        "# Online model\n",
        "online_state_dict, online_config, online_tokenizer_info = load_model_artifact(config['online_model_artifact'])\n",
        "print(f\"Loaded online model from {config['online_model_artifact']}\")\n",
        "\n",
        "# Offline model\n",
        "offline_state_dict, offline_config, offline_tokenizer_info = load_model_artifact(config['offline_model_artifact'])\n",
        "print(f\"Loaded offline model from {config['offline_model_artifact']}\")\n",
        "\n",
        "# Use tokenizer info from the online model (they should be identical)\n",
        "tokenizer_info = online_tokenizer_info\n",
        "print(\"Loaded tokenizer info from model artifacts\")\n",
        "\n",
        "# Verify tokenizer consistency\n",
        "if online_tokenizer_info != offline_tokenizer_info:\n",
        "    print(\"WARNING: Online and offline models have different tokenizer info!\")\n",
        "    print(\"This may cause evaluation issues.\")\n",
        "else:\n",
        "    print(\"âœ“ Tokenizer info consistent between models\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate Online Model\n",
        "print(\"\\n=== Evaluating Online Model with Diverse Sampling ===\")\n",
        "\n",
        "# Initialize model\n",
        "total_vocab_size = tokenizer_info['total_vocab_size']\n",
        "# Check for max_seq_length in config with common parameter names\n",
        "max_seq_length = 512\n",
        "\n",
        "# Fix vocabulary size if needed (should be 4665, not 4664)\n",
        "if total_vocab_size == 4664:\n",
        "    print(f\"WARNING: Adjusting vocab_size from {total_vocab_size} to 4665 to fix off-by-one error\")\n",
        "    total_vocab_size = 4665\n",
        "\n",
        "model = OnlineTransformer(\n",
        "    vocab_size=total_vocab_size,\n",
        "    embed_dim=online_config['embed_dim'],\n",
        "    num_heads=online_config['num_heads'],\n",
        "    num_layers=online_config['num_layers'],\n",
        "    dropout=online_config['dropout'],\n",
        "    max_seq_length=max_seq_length,\n",
        "    pad_token_id=PAD_TOKEN\n",
        ").to(device)\n",
        "\n",
        "print(f\"Initialized online model with max_seq_length: {max_seq_length}\")\n",
        "\n",
        "# Load state dict\n",
        "model.load_state_dict(online_state_dict)\n",
        "model.eval()\n",
        "\n",
        "# Create dataloader\n",
        "max_seq_length = online_config.get('max_seq_length') or online_config.get('max_sequence_length') or 512\n",
        "dataloader, _ = create_dataloader(\n",
        "    data_dir=Path(config['data_dir']),\n",
        "    split=config['split'],\n",
        "    batch_size=config['batch_size'],\n",
        "    num_workers=config['num_workers'],\n",
        "    sequence_length=max_seq_length,\n",
        "    mode='online',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Generate sequences with new sampling parameters\n",
        "print(\"\\nGenerating sequences with diverse sampling...\")\n",
        "generated_sequences, ground_truth_sequences = generate_online(\n",
        "    model=model,\n",
        "    dataloader=dataloader,\n",
        "    tokenizer_info=tokenizer_info,\n",
        "    device=device,\n",
        "    temperature=config['temperature'],\n",
        "    top_k=config['top_k'],\n",
        "    wait_beats=config['wait_beats']\n",
        ")\n",
        "\n",
        "# Debug: Check what format the generated sequences have\n",
        "print(f\"\\nDebug - Generated Sequences Format:\")\n",
        "print(f\"Number of sequences: {len(generated_sequences)}\")\n",
        "if generated_sequences:\n",
        "    print(f\"First sequence length: {len(generated_sequences[0])}\")\n",
        "    print(f\"First sequence sample: {generated_sequences[0][:10]}\")\n",
        "    print(f\"Token range: [{min(generated_sequences[0])}, {max(generated_sequences[0])}]\")\n",
        "    \n",
        "print(f\"\\nDebug - Ground Truth Format:\")\n",
        "if ground_truth_sequences:\n",
        "    print(f\"First GT sequence length: {len(ground_truth_sequences[0])}\")\n",
        "    print(f\"First GT sequence sample: {ground_truth_sequences[0][:10]}\")\n",
        "\n",
        "# Calculate metrics\n",
        "print(\"\\nCalculating metrics...\")\n",
        "harmony_metrics = calculate_harmony_metrics(generated_sequences, tokenizer_info)\n",
        "emd_metrics = calculate_emd_metrics(generated_sequences, ground_truth_sequences, tokenizer_info)\n",
        "\n",
        "online_metrics = {**harmony_metrics, **emd_metrics}\n",
        "\n",
        "print(\"\\n=== Online Model Results with Diverse Sampling ===\")\n",
        "for metric, value in online_metrics.items():\n",
        "    print(f\"{metric}: {value:.4f}\")\n",
        "    wandb.log({f\"online_diverse/{metric}\": value})\n",
        "    \n",
        "# Log the artifact version and sampling parameters\n",
        "wandb.log({\n",
        "    \"online_model_artifact\": config['online_model_artifact'],\n",
        "    \"sampling/temperature\": config['temperature'],\n",
        "    \"sampling/top_k\": config['top_k'],\n",
        "    \"sampling/wait_beats\": config['wait_beats']\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate Offline Model\n",
        "print(\"\\n=== Evaluating Offline Model with Diverse Sampling ===\")\n",
        "\n",
        "# Get max sequence length from config with common parameter names\n",
        "max_seq_length = (offline_config.get('max_seq_length') or \n",
        "                  offline_config.get('max_sequence_length') or \n",
        "                  256)  # Default to 256 for offline models\n",
        "\n",
        "# Initialize model\n",
        "model = T5OfflineTeacherModel(\n",
        "    melody_vocab_size=tokenizer_info['melody_vocab_size'],  # Get from tokenizer info\n",
        "    chord_vocab_size=tokenizer_info['chord_vocab_size'],    # Get from tokenizer info\n",
        "    embed_dim=offline_config['embed_dim'],\n",
        "    num_heads=offline_config['num_heads'],\n",
        "    num_layers=offline_config['num_layers'],\n",
        "    dropout=offline_config['dropout'],\n",
        "    max_seq_length=max_seq_length,\n",
        "    pad_token_id=PAD_TOKEN,\n",
        "    total_vocab_size=tokenizer_info.get('total_vocab_size', 4779)  # Use unified vocabulary\n",
        ").to(device)\n",
        "\n",
        "print(f\"Initialized offline model with max_seq_length: {max_seq_length}\")\n",
        "\n",
        "# Load state dict\n",
        "model.load_state_dict(offline_state_dict)\n",
        "model.eval()\n",
        "dataloader, _ = create_dataloader(\n",
        "    data_dir=Path(config['data_dir']),\n",
        "    split=config['split'],\n",
        "    batch_size=config['batch_size'],\n",
        "    num_workers=config['num_workers'],\n",
        "    sequence_length=max_seq_length,\n",
        "    mode='offline',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Generate sequences with new sampling parameters\n",
        "print(\"\\nGenerating sequences with diverse sampling...\")\n",
        "generated_sequences, ground_truth_sequences = generate_offline(\n",
        "    model=model,\n",
        "    dataloader=dataloader,\n",
        "    tokenizer_info=tokenizer_info,\n",
        "    device=device,\n",
        "    temperature=config['temperature'],\n",
        "    top_k=config['top_k']\n",
        ")\n",
        "\n",
        "# Calculate metrics\n",
        "print(\"\\nCalculating metrics...\")\n",
        "harmony_metrics = calculate_harmony_metrics(generated_sequences, tokenizer_info)\n",
        "emd_metrics = calculate_emd_metrics(generated_sequences, ground_truth_sequences, tokenizer_info)\n",
        "\n",
        "offline_metrics = {**harmony_metrics, **emd_metrics}\n",
        "\n",
        "print(\"\\n=== Offline Model Results with Diverse Sampling ===\")\n",
        "for metric, value in offline_metrics.items():\n",
        "    print(f\"{metric}: {value:.4f}\")\n",
        "    wandb.log({f\"offline_diverse/{metric}\": value})\n",
        "    \n",
        "# Log the artifact version and sampling parameters\n",
        "wandb.log({\n",
        "    \"offline_model_artifact\": config['offline_model_artifact'],\n",
        "    \"sampling/temperature\": config['temperature'],\n",
        "    \"sampling/top_k\": config['top_k']\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Debug: Examine what the offline model generated\n",
        "print(\"=== DEBUGGING OFFLINE MODEL GENERATION ===\")\n",
        "\n",
        "print(f\"\\nGenerated sequences info:\")\n",
        "print(f\"  Number of sequences: {len(generated_sequences)}\")\n",
        "print(f\"  Number of ground truth sequences: {len(ground_truth_sequences)}\")\n",
        "\n",
        "if generated_sequences:\n",
        "    print(f\"\\nFirst few generated sequences analysis:\")\n",
        "    for i, seq in enumerate(generated_sequences[:3]):\n",
        "        # Convert to numpy array if it's a list\n",
        "        if isinstance(seq, list):\n",
        "            seq_array = np.array(seq)\n",
        "        else:\n",
        "            seq_array = seq\n",
        "            \n",
        "        print(f\"\\nSequence {i}:\")\n",
        "        print(f\"  Length: {len(seq_array)}\")\n",
        "        print(f\"  Token range: [{seq_array.min()}, {seq_array.max()}]\")\n",
        "        print(f\"  Unique tokens: {len(np.unique(seq_array))}\")\n",
        "        print(f\"  First 10 tokens: {seq_array[:10]}\")\n",
        "        \n",
        "        # Check token distribution\n",
        "        unique_tokens, counts = np.unique(seq_array, return_counts=True)\n",
        "        print(f\"  Top 5 most common tokens:\")\n",
        "        for token, count in zip(unique_tokens[:5], counts[:5]):\n",
        "            print(f\"    Token {token}: {count} times ({count/len(seq_array)*100:.1f}%)\")\n",
        "        \n",
        "        # Check if mostly silence/PAD tokens\n",
        "        pad_token = tokenizer_info['pad_token_id']\n",
        "        pad_count = np.sum(seq_array == pad_token)\n",
        "        print(f\"  PAD token ({pad_token}) count: {pad_count} ({pad_count/len(seq_array)*100:.1f}%)\")\n",
        "\n",
        "# Check ground truth sequences too\n",
        "if ground_truth_sequences:\n",
        "    print(f\"\\nGround truth sequences analysis:\")\n",
        "    for i, seq in enumerate(ground_truth_sequences[:3]):\n",
        "        # Convert to numpy array if it's a list\n",
        "        if isinstance(seq, list):\n",
        "            seq_array = np.array(seq)\n",
        "        else:\n",
        "            seq_array = seq\n",
        "            \n",
        "        print(f\"\\nGround Truth Sequence {i}:\")\n",
        "        print(f\"  Length: {len(seq_array)}\")\n",
        "        print(f\"  Token range: [{seq_array.min()}, {seq_array.max()}]\")\n",
        "        print(f\"  Unique tokens: {len(np.unique(seq_array))}\")\n",
        "        print(f\"  First 10 tokens: {seq_array[:10]}\")\n",
        "        \n",
        "        # Check token distribution\n",
        "        unique_tokens, counts = np.unique(seq_array, return_counts=True)\n",
        "        print(f\"  Top 5 most common tokens:\")\n",
        "        for token, count in zip(unique_tokens[:5], counts[:5]):\n",
        "            print(f\"    Token {token}: {count} times ({count/len(seq_array)*100:.1f}%)\")\n",
        "\n",
        "# Show tokenizer info for reference\n",
        "print(f\"\\nTokenizer info for reference:\")\n",
        "print(f\"  Melody vocab size: {tokenizer_info['melody_vocab_size']}\")\n",
        "print(f\"  Chord vocab size: {tokenizer_info['chord_vocab_size']}\")\n",
        "print(f\"  Total vocab size: {tokenizer_info['total_vocab_size']}\")\n",
        "print(f\"  PAD token: {tokenizer_info['pad_token_id']}\")\n",
        "print(f\"  Melody tokens: 0 - {tokenizer_info['melody_vocab_size']-1}\")\n",
        "print(f\"  PAD token: {tokenizer_info['pad_token_id']}\")\n",
        "print(f\"  Chord tokens: {tokenizer_info['pad_token_id']+1} - {tokenizer_info['total_vocab_size']-1}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Debug: Test metrics parsing with generated sequences\n",
        "print(\"=== DEBUGGING METRICS PARSING ===\")\n",
        "\n",
        "# Test the parse_sequences function with our generated sequences\n",
        "print(f\"\\nTesting parse_sequences function...\")\n",
        "try:\n",
        "    sequences = list(parse_sequences(generated_sequences, tokenizer_info))\n",
        "    print(f\"Successfully parsed {len(sequences)} sequences\")\n",
        "    \n",
        "    if sequences:\n",
        "        print(f\"\\nFirst parsed sequence analysis:\")\n",
        "        seq_data = sequences[0]\n",
        "        print(f\"  Number of notes: {len(seq_data['notes'])}\")\n",
        "        print(f\"  Number of chords: {len(seq_data['chords'])}\")\n",
        "        \n",
        "        if seq_data['notes']:\n",
        "            print(f\"  First 3 notes:\")\n",
        "            for i, note in enumerate(seq_data['notes'][:3]):\n",
        "                print(f\"    Note {i}: {note}\")\n",
        "        else:\n",
        "            print(f\"  No notes found!\")\n",
        "            \n",
        "        if seq_data['chords']:\n",
        "            print(f\"  First 3 chords:\")\n",
        "            for i, chord in enumerate(seq_data['chords'][:3]):\n",
        "                print(f\"    Chord {i}: {chord}\")\n",
        "        else:\n",
        "            print(f\"  No chords found!\")\n",
        "            \n",
        "        # Check if we have the expected structure\n",
        "        if not seq_data['notes'] and not seq_data['chords']:\n",
        "            print(f\"  WARNING: No notes or chords found in parsed sequence!\")\n",
        "            \n",
        "    else:\n",
        "        print(f\"  No sequences were successfully parsed!\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"Error parsing sequences: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "# Debug the harmony calculation specifically\n",
        "print(f\"\\n=== DEBUGGING HARMONY CALCULATION ===\")\n",
        "try:\n",
        "    # Test harmony metrics calculation step by step\n",
        "    from src.evaluation.metrics import calculate_harmony_metrics\n",
        "    \n",
        "    # Create a small test with just a few sequences\n",
        "    test_sequences = generated_sequences[:5]\n",
        "    test_harmony_metrics = calculate_harmony_metrics(test_sequences, tokenizer_info)\n",
        "    \n",
        "    print(f\"Test harmony metrics (first 5 sequences):\")\n",
        "    for key, value in test_harmony_metrics.items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "        \n",
        "    # Show what tokens are being considered as melody vs chord\n",
        "    print(f\"\\nToken classification:\")\n",
        "    print(f\"  Melody token range: 0 to {tokenizer_info['melody_vocab_size']-1}\")\n",
        "    print(f\"  PAD token: {tokenizer_info['pad_token_id']}\")\n",
        "    print(f\"  Chord token range: {tokenizer_info['pad_token_id']+1} to {tokenizer_info['total_vocab_size']-1}\")\n",
        "    \n",
        "    # Check what tokens are actually in our sequences\n",
        "    if test_sequences:\n",
        "        # Convert sequences to arrays if they're lists\n",
        "        test_arrays = [np.array(seq) if isinstance(seq, list) else seq for seq in test_sequences]\n",
        "        all_tokens = np.concatenate(test_arrays)\n",
        "        unique_tokens = np.unique(all_tokens)\n",
        "        print(f\"\\nActual tokens in generated sequences:\")\n",
        "        print(f\"  Unique tokens: {unique_tokens}\")\n",
        "        print(f\"  Token counts:\")\n",
        "        for token in unique_tokens[:10]:  # Show first 10\n",
        "            count = np.sum(all_tokens == token)\n",
        "            if token < tokenizer_info['melody_vocab_size']:\n",
        "                token_type = \"MELODY\"\n",
        "            elif token == tokenizer_info['pad_token_id']:\n",
        "                token_type = \"PAD\"\n",
        "            else:\n",
        "                token_type = \"CHORD\"\n",
        "            print(f\"    Token {token} ({token_type}): {count} times\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Error in harmony calculation: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Debug: Compare with ground truth and identify the root cause\n",
        "print(\"=== DEBUGGING ROOT CAUSE ===\")\n",
        "\n",
        "# The issue is that offline model sequences are CHORD-ONLY\n",
        "# but the metrics expect INTERLEAVED melody+chord sequences\n",
        "print(f\"\\nROOT CAUSE ANALYSIS:\")\n",
        "print(f\"1. Online model generates INTERLEAVED sequences (melody + chord)\")\n",
        "print(f\"2. Offline model generates CHORD-ONLY sequences\")\n",
        "print(f\"3. Metrics parsing expects INTERLEAVED sequences\")\n",
        "\n",
        "# Check if this is true by examining sequence structure\n",
        "print(f\"\\nSequence structure comparison:\")\n",
        "print(f\"Generated sequences (offline):\")\n",
        "if generated_sequences:\n",
        "    seq = generated_sequences[0]\n",
        "    seq_array = np.array(seq) if isinstance(seq, list) else seq\n",
        "    print(f\"  Length: {len(seq_array)}\")\n",
        "    print(f\"  Token range: [{seq_array.min()}, {seq_array.max()}]\")\n",
        "    print(f\"  All tokens >= chord_start? {np.all(seq_array >= tokenizer_info['pad_token_id'])}\")\n",
        "\n",
        "print(f\"\\nGround truth sequences:\")\n",
        "if ground_truth_sequences:\n",
        "    seq = ground_truth_sequences[0]\n",
        "    seq_array = np.array(seq) if isinstance(seq, list) else seq\n",
        "    print(f\"  Length: {len(seq_array)}\")\n",
        "    print(f\"  Token range: [{seq_array.min()}, {seq_array.max()}]\")\n",
        "    print(f\"  All tokens >= chord_start? {np.all(seq_array >= tokenizer_info['pad_token_id'])}\")\n",
        "\n",
        "# Show the key insight\n",
        "print(f\"\\nKEY INSIGHT:\")\n",
        "print(f\"The offline model evaluation is trying to calculate melody-in-chord ratio\")\n",
        "print(f\"from CHORD-ONLY sequences, which is impossible!\")\n",
        "print(f\"\")\n",
        "print(f\"For offline evaluation, we need to:\")\n",
        "print(f\"1. Get the melody tokens from the input batch\")\n",
        "print(f\"2. Pair them with the generated chord tokens\")\n",
        "print(f\"3. Create interleaved sequences for metrics calculation\")\n",
        "\n",
        "# Check what the dataloader provides\n",
        "print(f\"\\nDataloader batch structure:\")\n",
        "print(f\"Available keys: {list(batch.keys()) if 'batch' in globals() else 'No batch loaded'}\")\n",
        "\n",
        "# Manual test of creating interleaved sequences\n",
        "print(f\"\\n=== TESTING MANUAL INTERLEAVED CREATION ===\")\n",
        "try:\n",
        "    # Create a small test dataloader to get melody tokens\n",
        "    from src.data.dataset import create_dataloader\n",
        "    \n",
        "    test_dataloader, _ = create_dataloader(\n",
        "        data_dir=Path(config['data_dir']),\n",
        "        split=config['split'],\n",
        "        batch_size=2,\n",
        "        num_workers=0,\n",
        "        sequence_length=256,\n",
        "        mode='offline',\n",
        "        shuffle=False\n",
        "    )\n",
        "    \n",
        "    # Get a batch\n",
        "    test_batch = next(iter(test_dataloader))\n",
        "    melody_tokens = test_batch['melody_tokens']\n",
        "    \n",
        "    print(f\"Melody tokens shape: {melody_tokens.shape}\")\n",
        "    print(f\"Melody token range: [{melody_tokens.min()}, {melody_tokens.max()}]\")\n",
        "    \n",
        "    # Create interleaved sequences manually\n",
        "    if generated_sequences:\n",
        "        print(f\"\\nCreating interleaved test sequence:\")\n",
        "        test_melody = melody_tokens[0].numpy()  # First melody\n",
        "        test_chords = generated_sequences[0]    # First generated chord sequence\n",
        "        test_chords = np.array(test_chords) if isinstance(test_chords, list) else test_chords\n",
        "        \n",
        "        # Ensure same length\n",
        "        min_len = min(len(test_melody), len(test_chords))\n",
        "        test_melody = test_melody[:min_len]\n",
        "        test_chords = test_chords[:min_len]\n",
        "        \n",
        "        # Create interleaved sequence\n",
        "        interleaved = np.empty(len(test_melody) * 2, dtype=np.int64)\n",
        "        interleaved[1::2] = test_melody  # Odd indices: melody\n",
        "        interleaved[0::2] = test_chords  # Even indices: chords\n",
        "        \n",
        "        print(f\"Interleaved sequence created:\")\n",
        "        print(f\"  Length: {len(interleaved)}\")\n",
        "        print(f\"  First 10 tokens: {interleaved[:10]}\")\n",
        "        \n",
        "        # Test metrics on this interleaved sequence\n",
        "        test_harmony = calculate_harmony_metrics([interleaved], tokenizer_info)\n",
        "        print(f\"Test harmony metrics on interleaved sequence:\")\n",
        "        for key, value in test_harmony.items():\n",
        "            print(f\"  {key}: {value}\")\n",
        "            \n",
        "except Exception as e:\n",
        "    print(f\"Error in manual test: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SOLUTION: Fix the offline evaluation to create proper interleaved sequences\n",
        "print(\"=== SOLUTION: FIXING OFFLINE EVALUATION ===\")\n",
        "\n",
        "print(f\"The issue is clear: offline model generates chord-only sequences,\")\n",
        "print(f\"but metrics expect interleaved melody+chord sequences.\")\n",
        "print(f\"\")\n",
        "print(f\"We need to modify the offline evaluation to:\")\n",
        "print(f\"1. Collect melody tokens from the input batches\")\n",
        "print(f\"2. Pair them with generated chord sequences\")\n",
        "print(f\"3. Create interleaved sequences for metrics\")\n",
        "\n",
        "# Let's create a fixed version of the evaluation manually\n",
        "print(f\"\\n=== MANUAL FIX FOR TESTING ===\")\n",
        "try:\n",
        "    # Re-run the offline generation but also collect melody tokens\n",
        "    print(\"Re-running offline generation with melody collection...\")\n",
        "    \n",
        "    # Store melody tokens from each batch\n",
        "    melody_sequences = []\n",
        "    \n",
        "    # We need to recreate the generation loop but collect melodies\n",
        "    generated_sequences_fixed = []\n",
        "    ground_truth_sequences_fixed = []\n",
        "    \n",
        "    # Get fresh dataloader\n",
        "    dataloader_fixed, _ = create_dataloader(\n",
        "        data_dir=Path(config['data_dir']),\n",
        "        split=config['split'],\n",
        "        batch_size=config['batch_size'],\n",
        "        num_workers=config['num_workers'],\n",
        "        sequence_length=max_seq_length,\n",
        "        mode='offline',\n",
        "        shuffle=False\n",
        "    )\n",
        "    \n",
        "    # We'll just use the existing sequences but pair them with melodies\n",
        "    batch_count = 0\n",
        "    for batch in dataloader_fixed:\n",
        "        if batch_count >= len(generated_sequences) // config['batch_size']:\n",
        "            break\n",
        "            \n",
        "        melody_tokens = batch['melody_tokens']\n",
        "        \n",
        "        # Add melody sequences for this batch\n",
        "        for i in range(melody_tokens.shape[0]):\n",
        "            melody_sequences.append(melody_tokens[i].numpy())\n",
        "        \n",
        "        batch_count += 1\n",
        "    \n",
        "    print(f\"Collected {len(melody_sequences)} melody sequences\")\n",
        "    print(f\"Generated {len(generated_sequences)} chord sequences\")\n",
        "    \n",
        "    # Create interleaved sequences for metrics\n",
        "    interleaved_sequences = []\n",
        "    \n",
        "    num_sequences = min(len(melody_sequences), len(generated_sequences))\n",
        "    print(f\"Creating {num_sequences} interleaved sequences...\")\n",
        "    \n",
        "    for i in range(num_sequences):\n",
        "        melody_seq = melody_sequences[i]\n",
        "        chord_seq = generated_sequences[i]\n",
        "        chord_seq = np.array(chord_seq) if isinstance(chord_seq, list) else chord_seq\n",
        "        \n",
        "        # Ensure same length\n",
        "        min_len = min(len(melody_seq), len(chord_seq))\n",
        "        melody_seq = melody_seq[:min_len]\n",
        "        chord_seq = chord_seq[:min_len]\n",
        "        \n",
        "        # Create interleaved sequence: [chord_0, melody_0, chord_1, melody_1, ...]\n",
        "        interleaved = np.empty(len(melody_seq) * 2, dtype=np.int64)\n",
        "        interleaved[1::2] = melody_seq  # Odd indices: melody\n",
        "        interleaved[0::2] = chord_seq   # Even indices: chords\n",
        "        \n",
        "        interleaved_sequences.append(interleaved)\n",
        "    \n",
        "    print(f\"Created {len(interleaved_sequences)} interleaved sequences\")\n",
        "    \n",
        "    # Now calculate metrics with proper interleaved sequences\n",
        "    print(f\"\\nCalculating metrics with fixed interleaved sequences...\")\n",
        "    \n",
        "    # Also create interleaved ground truth sequences\n",
        "    ground_truth_interleaved = []\n",
        "    for i in range(min(len(melody_sequences), len(ground_truth_sequences))):\n",
        "        melody_seq = melody_sequences[i]\n",
        "        gt_chord_seq = ground_truth_sequences[i]\n",
        "        gt_chord_seq = np.array(gt_chord_seq) if isinstance(gt_chord_seq, list) else gt_chord_seq\n",
        "        \n",
        "        min_len = min(len(melody_seq), len(gt_chord_seq))\n",
        "        melody_seq = melody_seq[:min_len]\n",
        "        gt_chord_seq = gt_chord_seq[:min_len]\n",
        "        \n",
        "        # Create interleaved ground truth\n",
        "        gt_interleaved = np.empty(len(melody_seq) * 2, dtype=np.int64)\n",
        "        gt_interleaved[1::2] = melody_seq    # Odd indices: melody\n",
        "        gt_interleaved[0::2] = gt_chord_seq  # Even indices: chords\n",
        "        \n",
        "        ground_truth_interleaved.append(gt_interleaved)\n",
        "    \n",
        "    # Calculate fixed metrics\n",
        "    fixed_harmony_metrics = calculate_harmony_metrics(interleaved_sequences, tokenizer_info)\n",
        "    fixed_emd_metrics = calculate_emd_metrics(interleaved_sequences, ground_truth_interleaved, tokenizer_info)\n",
        "    \n",
        "    print(f\"\\n=== FIXED OFFLINE MODEL RESULTS ===\")\n",
        "    for metric, value in {**fixed_harmony_metrics, **fixed_emd_metrics}.items():\n",
        "        print(f\"{metric}: {value:.4f}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Error in manual fix: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "print(f\"\\nTo permanently fix this, we need to modify the generate_offline function\")\n",
        "print(f\"to return interleaved sequences instead of chord-only sequences.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
