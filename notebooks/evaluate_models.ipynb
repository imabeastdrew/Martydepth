{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!git clone https://github.com/imabeastdrew/Martydepth.git\n",
        "%cd Martydepth\n",
        "\n",
        "# Install the package in development mode\n",
        "%pip install -e .\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "%pip install torch wandb tqdm pyyaml\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import torch\n",
        "import wandb\n",
        "import tempfile\n",
        "import json\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from tqdm.notebook import tqdm\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Add project root to Python path\n",
        "sys.path.append('.')\n",
        "\n",
        "print(os.getcwd())\n",
        "\n",
        "# Import project modules\n",
        "from src.data.dataset import create_dataloader\n",
        "from src.models.online_transformer import OnlineTransformer\n",
        "from src.models.offline_teacher import OfflineTeacherModel\n",
        "from src.evaluation.metrics import (\n",
        "    calculate_harmony_metrics,\n",
        "    calculate_emd_metrics,\n",
        ")\n",
        "from src.evaluation.evaluate import load_model_from_wandb as load_online_model\n",
        "from src.evaluation.evaluate_offline import load_model_from_wandb as load_offline_model\n",
        "from src.evaluation.evaluate import generate_online\n",
        "from src.evaluation.evaluate_offline import generate_offline\n",
        "from src.config.tokenization_config import PAD_TOKEN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "config = {\n",
        "    'data_dir': 'data/interim',\n",
        "    'split': 'test',\n",
        "    'batch_size': 32,\n",
        "    'num_workers': 4,\n",
        "    'temperature': 1.0,\n",
        "    'top_k': 50,\n",
        "}\n",
        "\n",
        "# Model artifact paths (replace with your actual artifact paths)\n",
        "online_artifact_path = \"marty1ai/martydepth/celestial-field-49-epoch-34:v0\"\n",
        "offline_artifact_path = \"marty1ai/martydepth/offline_teacher_epoch_19_loss_0.23:v0\"\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \n",
        "                     \"mps\" if torch.backends.mps.is_available() else \n",
        "                     \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Initialize wandb\n",
        "wandb.init(\n",
        "    project=\"martydepth\",\n",
        "    name=\"model_evaluation\",\n",
        "    config=config,\n",
        "    job_type=\"evaluation\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate Online Model\n",
        "print(\"\\n=== Evaluating Online Model ===\")\n",
        "print(f\"Loading model from artifact: {online_artifact_path}\")\n",
        "\n",
        "# Load model\n",
        "model, tokenizer_info, model_config = load_online_model(online_artifact_path, device)\n",
        "model.eval()\n",
        "\n",
        "# Create dataloader\n",
        "max_seq_length = model_config.get('max_seq_length') or model_config.get('max_sequence_length') or 512\n",
        "dataloader, _ = create_dataloader(\n",
        "    data_dir=Path(config['data_dir']),\n",
        "    split=config['split'],\n",
        "    batch_size=config['batch_size'],\n",
        "    num_workers=config['num_workers'],\n",
        "    sequence_length=max_seq_length,\n",
        "    mode='online',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Generate sequences\n",
        "print(\"\\nGenerating sequences...\")\n",
        "generated_sequences, ground_truth_sequences = generate_online(\n",
        "    model=model,\n",
        "    dataloader=dataloader,\n",
        "    tokenizer_info=tokenizer_info,\n",
        "    device=device,\n",
        "    temperature=config['temperature'],\n",
        "    top_k=config['top_k']\n",
        ")\n",
        "\n",
        "# Calculate metrics\n",
        "print(\"\\nCalculating metrics...\")\n",
        "harmony_metrics = calculate_harmony_metrics(generated_sequences, tokenizer_info)\n",
        "emd_metrics = calculate_emd_metrics(generated_sequences, ground_truth_sequences, tokenizer_info)\n",
        "\n",
        "online_metrics = {**harmony_metrics, **emd_metrics}\n",
        "\n",
        "print(\"\\n=== Online Model Results ===\")\n",
        "for metric, value in online_metrics.items():\n",
        "    print(f\"{metric}: {value:.4f}\")\n",
        "    wandb.log({f\"online/{metric}\": value})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate Offline Model\n",
        "print(\"\\n=== Evaluating Offline Model ===\")\n",
        "print(f\"Loading model from artifact: {offline_artifact_path}\")\n",
        "\n",
        "# Load model\n",
        "model, model_config, tokenizer_info = load_offline_model(offline_artifact_path, device)\n",
        "model.eval()\n",
        "\n",
        "# Create dataloader\n",
        "max_seq_length = model_config.get('max_seq_length') or model_config.get('max_sequence_length') or 512\n",
        "dataloader, _ = create_dataloader(\n",
        "    data_dir=Path(config['data_dir']),\n",
        "    split=config['split'],\n",
        "    batch_size=config['batch_size'],\n",
        "    num_workers=config['num_workers'],\n",
        "    sequence_length=max_seq_length,\n",
        "    mode='offline',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Generate sequences\n",
        "print(\"\\nGenerating sequences...\")\n",
        "generated_sequences, ground_truth_sequences = generate_offline(\n",
        "    model=model,\n",
        "    dataloader=dataloader,\n",
        "    tokenizer_info=tokenizer_info,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "# Calculate metrics\n",
        "print(\"\\nCalculating metrics...\")\n",
        "harmony_metrics = calculate_harmony_metrics(generated_sequences, tokenizer_info)\n",
        "emd_metrics = calculate_emd_metrics(generated_sequences, ground_truth_sequences, tokenizer_info)\n",
        "\n",
        "offline_metrics = {**harmony_metrics, **emd_metrics}\n",
        "\n",
        "print(\"\\n=== Offline Model Results ===\")\n",
        "for metric, value in offline_metrics.items():\n",
        "    print(f\"{metric}: {value:.4f}\")\n",
        "    wandb.log({f\"offline/{metric}\": value})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare Results\n",
        "print(\"\\n=== Model Comparison ===\")\n",
        "print(f\"{'Metric':<30} {'Online':<10} {'Offline':<10} {'Difference':<10}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for metric in online_metrics.keys():\n",
        "    online_value = online_metrics[metric]\n",
        "    offline_value = offline_metrics[metric]\n",
        "    diff = online_value - offline_value\n",
        "    print(f\"{metric:<30} {online_value:>10.4f} {offline_value:>10.4f} {diff:>10.4f}\")\n",
        "    wandb.log({\n",
        "        f\"comparison/{metric}_diff\": diff,\n",
        "        f\"comparison/{metric}_ratio\": online_value / offline_value if offline_value != 0 else 0\n",
        "    })\n",
        "\n",
        "# Finish wandb run\n",
        "wandb.finish()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
