{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!git clone https://github.com/imabeastdrew/Martydepth.git\n",
        "%cd Martydepth\n",
        "\n",
        "# Install the package in development mode\n",
        "%pip install -e .\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "%pip install torch wandb tqdm pyyaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import torch\n",
        "import wandb\n",
        "import json\n",
        "import numpy as np\n",
        "import tempfile\n",
        "from pathlib import Path\n",
        "from tqdm.notebook import tqdm\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Add project root to Python path\n",
        "sys.path.append('.')\n",
        "\n",
        "# Import project modules\n",
        "from src.data.dataset import create_dataloader\n",
        "from src.models.online_transformer import OnlineTransformer\n",
        "from src.models.offline_teacher import OfflineTeacherModel\n",
        "from src.evaluation.metrics import (\n",
        "    calculate_harmony_metrics,\n",
        "    calculate_emd_metrics,\n",
        "    parse_sequences,  # Added this import\n",
        ")\n",
        "from src.config.tokenization_config import (\n",
        "    SILENCE_TOKEN,\n",
        "    MELODY_ONSET_HOLD_START,\n",
        "    CHORD_TOKEN_START,\n",
        "    PAD_TOKEN,\n",
        ")\n",
        "from src.evaluation.evaluate_offline import generate_offline\n",
        "from src.evaluation.evaluate import generate_online\n",
        "\n",
        "def load_model_artifact(artifact_path: str) -> tuple[dict, dict, dict]:\n",
        "    \"\"\"\n",
        "    Load a model artifact and its config from wandb.\n",
        "    Compatible with both checkpoint-style and separate file artifacts.\n",
        "    \n",
        "    Args:\n",
        "        artifact_path: Full path to the artifact (e.g. 'marty1ai/martydepth/model_name:version')\n",
        "    \n",
        "    Returns:\n",
        "        tuple[dict, dict, dict]: Model state dict, config, and tokenizer_info\n",
        "    \"\"\"\n",
        "    api = wandb.Api()\n",
        "    artifact = api.artifact(artifact_path)\n",
        "    \n",
        "    # Download the artifact\n",
        "    with tempfile.TemporaryDirectory() as tmp_dir:\n",
        "        artifact_dir = artifact.download(tmp_dir)\n",
        "        artifact_path_obj = Path(artifact_dir)\n",
        "        \n",
        "        # Check for different artifact structures\n",
        "        checkpoint_files = list(artifact_path_obj.glob(\"*.pth\"))\n",
        "        tokenizer_files = list(artifact_path_obj.glob(\"tokenizer_info.json\"))\n",
        "        \n",
        "        if len(checkpoint_files) == 1 and len(tokenizer_files) == 1:\n",
        "            # Separate files structure (model.pth + tokenizer_info.json)\n",
        "            model_path = checkpoint_files[0]\n",
        "            tokenizer_path = tokenizer_files[0]\n",
        "            \n",
        "            # Load model state dict directly\n",
        "            model_state_dict = torch.load(model_path, map_location='cpu', weights_only=True)\n",
        "            \n",
        "            # Load tokenizer info\n",
        "            with open(tokenizer_path, 'r') as f:\n",
        "                tokenizer_info = json.load(f)\n",
        "            \n",
        "            # Get config from the run that created this artifact\n",
        "            run = artifact.logged_by()\n",
        "            config = dict(run.config)\n",
        "            \n",
        "        elif len(checkpoint_files) == 1 and len(tokenizer_files) == 0:\n",
        "            # Checkpoint structure (single .pth file with everything)\n",
        "            checkpoint_file = checkpoint_files[0]\n",
        "            checkpoint = torch.load(checkpoint_file, map_location='cpu')\n",
        "            \n",
        "            if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:\n",
        "                # Training checkpoint structure\n",
        "                model_state_dict = checkpoint['model_state_dict']\n",
        "                config = checkpoint.get('config', {})\n",
        "                \n",
        "                # Try to get tokenizer info from run config or load from data dir\n",
        "                run = artifact.logged_by()\n",
        "                run_config = dict(run.config)\n",
        "                config.update(run_config)  # Merge configs\n",
        "                \n",
        "                # Load tokenizer info from data directory as fallback\n",
        "                tokenizer_info_path = Path(\"data/interim/train/tokenizer_info.json\")\n",
        "                if tokenizer_info_path.exists():\n",
        "                    with open(tokenizer_info_path, 'r') as f:\n",
        "                        tokenizer_info = json.load(f)\n",
        "                else:\n",
        "                    raise FileNotFoundError(\"Could not find tokenizer_info.json in checkpoint or data directory\")\n",
        "            else:\n",
        "                # Direct state dict\n",
        "                model_state_dict = checkpoint\n",
        "                run = artifact.logged_by()\n",
        "                config = dict(run.config)\n",
        "                \n",
        "                # Load tokenizer info from data directory\n",
        "                tokenizer_info_path = Path(\"data/interim/train/tokenizer_info.json\")\n",
        "                with open(tokenizer_info_path, 'r') as f:\n",
        "                    tokenizer_info = json.load(f)\n",
        "        else:\n",
        "            raise ValueError(f\"Unexpected artifact structure in {artifact_dir}. Expected either model.pth+tokenizer_info.json or single checkpoint.pth\")\n",
        "    \n",
        "    return model_state_dict, config, tokenizer_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "config = {\n",
        "    # Data parameters\n",
        "    'data_dir': 'data/interim',\n",
        "    'split': 'test',\n",
        "    'batch_size': 32,\n",
        "    'num_workers': 4,\n",
        "    \n",
        "    # Sampling parameters\n",
        "    'temperature': 1.0,     # Increased from 1.3 for more exploration\n",
        "    'top_k': 50,           # Reduced from 30 to focus on more likely but still diverse options\n",
        "    'wait_beats': 1,       # double\n",
        "    \n",
        "    # Model artifact paths\n",
        "    'online_model_artifact': 'marty1ai/martydepth/online_transformer_model_vercmpgd:v12',\n",
        "    'offline_model_artifact': 'marty1ai/martydepth/offline_teacher_model_2hd3b6gi:v8'\n",
        "}\n",
        "\n",
        "# Add some helpful frame/beat conversions\n",
        "frames_per_beat = 4  # Standard in our dataset\n",
        "\n",
        "print(f\"\\nSampling Parameters:\")\n",
        "print(f\"Temperature: {config['temperature']}\")\n",
        "print(f\"Top-k: {config['top_k']}\")\n",
        "print(f\"Wait beats: {config['wait_beats']}\")\n",
        "\n",
        "# Import necessary constants\n",
        "from src.config.tokenization_config import (\n",
        "    MELODY_VOCAB_SIZE,\n",
        "    CHORD_TOKEN_START,\n",
        "    SILENCE_TOKEN,\n",
        "    PAD_TOKEN\n",
        ")\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else\n",
        "                     \"mps\" if torch.backends.mps.is_available() else\n",
        "                     \"cpu\")\n",
        "print(f\"\\nUsing device: {device}\")\n",
        "\n",
        "# Initialize wandb\n",
        "wandb.init(\n",
        "    project=\"martydepth\",\n",
        "    name=\"model_evaluation_configurable_durations\",  # Updated name to reflect changes\n",
        "    config=config,\n",
        "    job_type=\"evaluation\"\n",
        ")\n",
        "\n",
        "# Load model artifacts\n",
        "print(\"\\nLoading model artifacts...\")\n",
        "\n",
        "# Online model\n",
        "online_state_dict, online_config, online_tokenizer_info = load_model_artifact(config['online_model_artifact'])\n",
        "print(f\"Loaded online model from {config['online_model_artifact']}\")\n",
        "\n",
        "# Offline model\n",
        "offline_state_dict, offline_config, offline_tokenizer_info = load_model_artifact(config['offline_model_artifact'])\n",
        "print(f\"Loaded offline model from {config['offline_model_artifact']}\")\n",
        "\n",
        "# Use tokenizer info from the online model (they should be identical)\n",
        "tokenizer_info = online_tokenizer_info\n",
        "print(\"Loaded tokenizer info from model artifacts\")\n",
        "\n",
        "# Verify tokenizer consistency\n",
        "if online_tokenizer_info != offline_tokenizer_info:\n",
        "    print(\"WARNING: Online and offline models have different tokenizer info!\")\n",
        "    print(\"This may cause evaluation issues.\")\n",
        "else:\n",
        "    print(\"âœ“ Tokenizer info consistent between models\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate Online Model\n",
        "print(\"\\n=== Evaluating Online Model with Diverse Sampling ===\")\n",
        "\n",
        "# Initialize model\n",
        "total_vocab_size = tokenizer_info['total_vocab_size']\n",
        "# Check for max_seq_length in config with common parameter names\n",
        "max_seq_length = 512\n",
        "\n",
        "model = OnlineTransformer(\n",
        "    vocab_size=total_vocab_size,\n",
        "    embed_dim=online_config['embed_dim'],\n",
        "    num_heads=online_config['num_heads'],\n",
        "    num_layers=online_config['num_layers'],\n",
        "    dropout=online_config['dropout'],\n",
        "    max_seq_length=max_seq_length,\n",
        "    pad_token_id=PAD_TOKEN\n",
        ").to(device)\n",
        "\n",
        "print(f\"Initialized online model with max_seq_length: {max_seq_length}\")\n",
        "\n",
        "# Load state dict\n",
        "model.load_state_dict(online_state_dict)\n",
        "model.eval()\n",
        "\n",
        "# Create dataloader\n",
        "max_seq_length = online_config.get('max_seq_length') or online_config.get('max_sequence_length') or 512\n",
        "dataloader, _ = create_dataloader(\n",
        "    data_dir=Path(config['data_dir']),\n",
        "    split=config['split'],\n",
        "    batch_size=config['batch_size'],\n",
        "    num_workers=config['num_workers'],\n",
        "    sequence_length=max_seq_length,\n",
        "    mode='online',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Generate sequences with new sampling parameters\n",
        "print(\"\\nGenerating sequences with diverse sampling...\")\n",
        "generated_sequences, ground_truth_sequences = generate_online(\n",
        "    model=model,\n",
        "    dataloader=dataloader,\n",
        "    tokenizer_info=tokenizer_info,\n",
        "    device=device,\n",
        "    temperature=config['temperature'],\n",
        "    top_k=config['top_k'],\n",
        "    wait_beats=config['wait_beats']\n",
        ")\n",
        "\n",
        "# Debug: Check what format the generated sequences have\n",
        "print(f\"\\nDebug - Generated Sequences Format:\")\n",
        "print(f\"Number of sequences: {len(generated_sequences)}\")\n",
        "if generated_sequences:\n",
        "    print(f\"First sequence length: {len(generated_sequences[0])}\")\n",
        "    print(f\"First sequence sample: {generated_sequences[0][:10]}\")\n",
        "    print(f\"Token range: [{min(generated_sequences[0])}, {max(generated_sequences[0])}]\")\n",
        "    \n",
        "print(f\"\\nDebug - Ground Truth Format:\")\n",
        "if ground_truth_sequences:\n",
        "    print(f\"First GT sequence length: {len(ground_truth_sequences[0])}\")\n",
        "    print(f\"First GT sequence sample: {ground_truth_sequences[0][:10]}\")\n",
        "\n",
        "# Calculate metrics\n",
        "print(\"\\nCalculating metrics...\")\n",
        "harmony_metrics = calculate_harmony_metrics(generated_sequences, tokenizer_info)\n",
        "emd_metrics = calculate_emd_metrics(generated_sequences, ground_truth_sequences, tokenizer_info)\n",
        "\n",
        "online_metrics = {**harmony_metrics, **emd_metrics}\n",
        "\n",
        "print(\"\\n=== Online Model Results with Diverse Sampling ===\")\n",
        "for metric, value in online_metrics.items():\n",
        "    print(f\"{metric}: {value:.4f}\")\n",
        "    wandb.log({f\"online_diverse/{metric}\": value})\n",
        "    \n",
        "# Log the artifact version and sampling parameters\n",
        "wandb.log({\n",
        "    \"online_model_artifact\": config['online_model_artifact'],\n",
        "    \"sampling/temperature\": config['temperature'],\n",
        "    \"sampling/top_k\": config['top_k'],\n",
        "    \"sampling/wait_beats\": config['wait_beats']\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate Offline Model\n",
        "print(\"\\n=== Evaluating Offline Model with Diverse Sampling ===\")\n",
        "\n",
        "# Get max sequence length from config with common parameter names\n",
        "max_seq_length = (offline_config.get('max_seq_length') or \n",
        "                  offline_config.get('max_sequence_length') or \n",
        "                  256)  # Default to 256 for offline models\n",
        "\n",
        "# Initialize model\n",
        "model = OfflineTeacherModel(\n",
        "    melody_vocab_size=tokenizer_info['melody_vocab_size'],  # Get from tokenizer info\n",
        "    chord_vocab_size=tokenizer_info['chord_vocab_size'],    # Get from tokenizer info\n",
        "    embed_dim=offline_config['embed_dim'],\n",
        "    num_heads=offline_config['num_heads'],\n",
        "    num_layers=offline_config['num_layers'],\n",
        "    dropout=offline_config['dropout'],\n",
        "    max_seq_length=max_seq_length,\n",
        "    pad_token_id=PAD_TOKEN\n",
        ").to(device)\n",
        "\n",
        "print(f\"Initialized offline model with max_seq_length: {max_seq_length}\")\n",
        "\n",
        "# Load state dict\n",
        "model.load_state_dict(offline_state_dict)\n",
        "model.eval()\n",
        "dataloader, _ = create_dataloader(\n",
        "    data_dir=Path(config['data_dir']),\n",
        "    split=config['split'],\n",
        "    batch_size=config['batch_size'],\n",
        "    num_workers=config['num_workers'],\n",
        "    sequence_length=max_seq_length,\n",
        "    mode='offline',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Generate sequences with new sampling parameters\n",
        "print(\"\\nGenerating sequences with diverse sampling...\")\n",
        "generated_sequences, ground_truth_sequences = generate_offline(\n",
        "    model=model,\n",
        "    dataloader=dataloader,\n",
        "    tokenizer_info=tokenizer_info,\n",
        "    device=device,\n",
        "    temperature=config['temperature'],\n",
        "    top_k=config['top_k']\n",
        ")\n",
        "\n",
        "# Calculate metrics\n",
        "print(\"\\nCalculating metrics...\")\n",
        "harmony_metrics = calculate_harmony_metrics(generated_sequences, tokenizer_info)\n",
        "emd_metrics = calculate_emd_metrics(generated_sequences, ground_truth_sequences, tokenizer_info)\n",
        "\n",
        "offline_metrics = {**harmony_metrics, **emd_metrics}\n",
        "\n",
        "print(\"\\n=== Offline Model Results with Diverse Sampling ===\")\n",
        "for metric, value in offline_metrics.items():\n",
        "    print(f\"{metric}: {value:.4f}\")\n",
        "    wandb.log({f\"offline_diverse/{metric}\": value})\n",
        "    \n",
        "# Log the artifact version and sampling parameters\n",
        "wandb.log({\n",
        "    \"offline_model_artifact\": config['offline_model_artifact'],\n",
        "    \"sampling/temperature\": config['temperature'],\n",
        "    \"sampling/top_k\": config['top_k']\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Recalculate metrics from previous sequences\n",
        "print(\"\\n=== Recalculating Metrics from Previous Run ===\")\n",
        "\n",
        "# Recalculate metrics using the sequences we already generated\n",
        "harmony_metrics = calculate_harmony_metrics(generated_sequences, tokenizer_info)\n",
        "emd_metrics = calculate_emd_metrics(generated_sequences, ground_truth_sequences, tokenizer_info)\n",
        "\n",
        "print(\"\\n=== Online Model Results ===\")\n",
        "for metric, value in {**harmony_metrics, **emd_metrics}.items():\n",
        "    print(f\"{metric}: {value:.4f}\")\n",
        "\n",
        "# Print raw histograms for debugging\n",
        "print(\"\\n=== Debug: Raw Histograms ===\")\n",
        "\n",
        "# Get all sequences first since parse_sequences is a generator\n",
        "sequences = list(parse_sequences(generated_sequences, tokenizer_info))\n",
        "\n",
        "# Get intervals and print debug info for first few sequences\n",
        "intervals = []\n",
        "debug_count = 0\n",
        "for data in sequences:\n",
        "    if not data['notes'] or not data['chords']: continue\n",
        "    \n",
        "    # Print debug info for first sequence with both notes and chords\n",
        "    if debug_count < 1:\n",
        "        print(\"\\nExample sequence structure:\")\n",
        "        print(f\"Number of notes: {len(data['notes'])}\")\n",
        "        print(f\"Number of chords: {len(data['chords'])}\")\n",
        "        print(\"\\nFirst few notes:\")\n",
        "        for note in data['notes'][:3]:\n",
        "            print(f\"Note: {note}\")\n",
        "        print(\"\\nFirst few chords:\")\n",
        "        for chord in data['chords'][:3]:\n",
        "            print(f\"Chord: {chord}\")\n",
        "        debug_count += 1\n",
        "    \n",
        "    note_onsets = np.array([n['start'] for n in data['notes']])\n",
        "    chord_onsets = np.array([c['start'] for c in data['chords']])\n",
        "    \n",
        "    # Debug first few interval calculations\n",
        "    if debug_count == 1:\n",
        "        print(\"\\nFirst few interval calculations:\")\n",
        "        for i, n_onset in enumerate(note_onsets[:3]):\n",
        "            if len(chord_onsets) > 0:\n",
        "                # Find closest chord onset using absolute distance\n",
        "                distances = np.abs(chord_onsets - n_onset)\n",
        "                min_distance = np.min(distances)\n",
        "                closest_idx = np.argmin(distances)\n",
        "                closest_onset = chord_onsets[closest_idx]\n",
        "                print(f\"Note onset {n_onset}, closest chord onset: {closest_onset}, absolute distance: {min_distance} frames\")\n",
        "    \n",
        "    # Calculate all intervals\n",
        "    for n_onset in note_onsets:\n",
        "        if len(chord_onsets) > 0:\n",
        "            # Find closest chord onset using absolute distance\n",
        "            distances = np.abs(chord_onsets - n_onset)\n",
        "            min_distance = np.min(distances)\n",
        "            intervals.append(min_distance)\n",
        "\n",
        "if not intervals:\n",
        "    print(\"No interval data found!\")\n",
        "else:\n",
        "    # Print raw interval statistics\n",
        "    print(f\"\\nInterval statistics:\")\n",
        "    print(f\"Min interval: {min(intervals)}\")\n",
        "    print(f\"Max interval: {max(intervals)}\")\n",
        "    print(f\"Mean interval: {np.mean(intervals):.2f}\")\n",
        "    print(f\"Median interval: {np.median(intervals):.2f}\")\n",
        "    \n",
        "    # Create bins as specified in paper [0, 1, 2, ..., 16, 17, âˆž]\n",
        "    onset_bins = list(range(18)) + [np.inf]\n",
        "    # Use weights=None to get raw counts first\n",
        "    hist, bin_edges = np.histogram(intervals, bins=onset_bins, weights=None)\n",
        "    # Then normalize manually to get probabilities\n",
        "    hist = hist / np.sum(hist) if np.sum(hist) > 0 else hist\n",
        "\n",
        "    print(\"\\nOnset Interval Distribution:\")\n",
        "    print(f\"Total intervals: {len(intervals)}\")\n",
        "    print(f\"Bin edges: {bin_edges}\")\n",
        "    print(\"\\nIntervals show absolute distance between note and nearest chord onset\")\n",
        "    print(\"0 means note and chord are simultaneous\")\n",
        "    print(\"Higher values mean more frames between note and nearest chord\")\n",
        "    print(\"\")\n",
        "    \n",
        "    for i, count in enumerate(hist):\n",
        "        if i < len(hist)-1:\n",
        "            print(f\"Bin {i}: {count:.4f} ({int(count * len(intervals))} intervals)\")\n",
        "        else:\n",
        "            print(f\"Bin >17: {count:.4f} ({int(count * len(intervals))} intervals)\")\n",
        "\n",
        "# Finish wandb run\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate chord length distribution\n",
        "print(\"\\n=== Debug: Chord Length Distribution ===\")\n",
        "chord_lengths = []\n",
        "\n",
        "# Get first sequence to examine chord structure\n",
        "sequences = list(parse_sequences(generated_sequences, tokenizer_info))\n",
        "if sequences and sequences[0]['chords']:\n",
        "    print(\"\\nExample chord structure:\")\n",
        "    print(sequences[0]['chords'][0])\n",
        "\n",
        "# Process all sequences\n",
        "for data in sequences:\n",
        "    if not data['chords']: continue\n",
        "    for chord in data['chords']:\n",
        "        # Calculate length as end - start\n",
        "        chord_lengths.append(chord['end'] - chord['start'])\n",
        "\n",
        "if not chord_lengths:\n",
        "    print(\"No chord data found!\")\n",
        "else:\n",
        "    print(f\"\\nTotal chords: {len(chord_lengths)}\")\n",
        "    print(f\"Length range: [{min(chord_lengths)}, {max(chord_lengths)}]\")\n",
        "\n",
        "    # Print histogram counts\n",
        "    length_bins = list(range(34)) + [np.inf]  # [0,1,2,...,32,33,âˆž]\n",
        "    # Use weights=None to get raw counts first\n",
        "    hist, _ = np.histogram(chord_lengths, bins=length_bins, weights=None)\n",
        "    # Then normalize manually to get probabilities\n",
        "    hist = hist / np.sum(hist) if np.sum(hist) > 0 else hist\n",
        "\n",
        "    print(\"\\nChord Length Distribution:\")\n",
        "    for i, count in enumerate(hist):\n",
        "        if i < len(hist)-1:\n",
        "            print(f\"Bin {i}: {count:.4f} ({int(count * len(chord_lengths))} chords)\")\n",
        "        else:\n",
        "            print(f\"Bin >33: {count:.4f} ({int(count * len(chord_lengths))} chords)\")\n",
        "\n",
        "    # Calculate entropy from the normalized histogram\n",
        "    entropy = -np.sum(hist[hist > 0] * np.log(hist[hist > 0]))  # Calculate entropy in nats\n",
        "    print(f\"\\nChord Length Entropy: {entropy:.4f} nats\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
