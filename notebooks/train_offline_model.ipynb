{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!git clone https://github.com/imabeastdrew/Martydepth.git\n",
        "%cd Martydepth\n",
        "\n",
        "# Install the package in development mode\n",
        "%pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "%pip install torch wandb tqdm pyyaml transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "from transformers import Adafactor\n",
        "from pathlib import Path\n",
        "import wandb\n",
        "import yaml\n",
        "import json\n",
        "from tqdm.notebook import tqdm\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Add project root to Python path\n",
        "import sys\n",
        "sys.path.append('.')\n",
        "\n",
        "print(os.getcwd())\n",
        "\n",
        "# Import project modules\n",
        "from src.data.dataset import create_dataloader\n",
        "from src.models.offline_teacher_t5 import T5OfflineTeacherModel\n",
        "from src.config.tokenization_config import PAD_TOKEN\n",
        "from src.training.utils.schedulers import get_warmup_schedule\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load configuration\n",
        "print(os.getcwd())\n",
        "config_path = 'src/training/configs/offline_teacher_base.yaml'\n",
        "with open(config_path, 'r') as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "# Initialize wandb\n",
        "run_name = f\"offline_L{config['num_layers']}_H{config['num_heads']}_D{config['embed_dim']}_seq{config['max_sequence_length']}_bs{config['batch_size']}_lr{config['learning_rate']}\"\n",
        "\n",
        "wandb.init(\n",
        "    project=config['wandb_project'],\n",
        "    name=run_name,\n",
        "    config=config,\n",
        "    job_type=\"offline_training\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create dataloaders\n",
        "train_loader, tokenizer_info = create_dataloader(\n",
        "    data_dir=Path(config['data_dir']),\n",
        "    split=\"train\",\n",
        "    batch_size=config['batch_size'],\n",
        "    num_workers=config['num_workers'],\n",
        "    sequence_length=config['max_sequence_length'],\n",
        "    mode='offline',\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_loader, _ = create_dataloader(\n",
        "    data_dir=Path(config['data_dir']),\n",
        "    split=\"valid\",\n",
        "    batch_size=config['batch_size'],\n",
        "    num_workers=config['num_workers'],\n",
        "    sequence_length=config['max_sequence_length'],\n",
        "    mode='offline',\n",
        "    shuffle=False\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display model information\n",
        "print(\"=== T5 Offline Teacher Model Info ===\")\n",
        "model_info = model.get_model_info()\n",
        "for key, value in model_info.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "\n",
        "print(f\"\\n=== Vocabulary Configuration ===\")\n",
        "print(f\"  Melody vocab size: {melody_vocab_size}\")\n",
        "print(f\"  Chord vocab size: {chord_vocab_size}\")\n",
        "print(f\"  PAD token ID: {pad_token_id}\")\n",
        "print(f\"  Tokenizer info - melody: {tokenizer_info['melody_vocab_size']}\")\n",
        "print(f\"  Tokenizer info - chord: {tokenizer_info['chord_vocab_size']}\")\n",
        "\n",
        "# Verify model can handle a forward pass\n",
        "print(f\"\\n=== Testing Model Forward Pass ===\")\n",
        "sample_batch = next(iter(train_loader))\n",
        "with torch.no_grad():\n",
        "    melody_tokens = sample_batch['melody_tokens'][:2].to(device)  # Use small batch\n",
        "    chord_input = sample_batch['chord_input'][:2].to(device)\n",
        "    \n",
        "    print(f\"  Input shapes - Melody: {melody_tokens.shape}, Chord: {chord_input.shape}\")\n",
        "    print(f\"  Token ranges - Melody: {melody_tokens.min()}-{melody_tokens.max()}, Chord: {chord_input.min()}-{chord_input.max()}\")\n",
        "    \n",
        "    # Test forward pass\n",
        "    logits = model(melody_tokens, chord_input)\n",
        "    print(f\"  Output shape: {logits.shape}\")\n",
        "    print(f\"  âœ… Forward pass successful!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize model, optimizer, criterion, and scheduler\n",
        "# Use unified vocabulary approach - T5 handles all tokens in one embedding space\n",
        "melody_vocab_size = tokenizer_info['melody_vocab_size'] \n",
        "chord_vocab_size = tokenizer_info['chord_vocab_size'] \n",
        "total_vocab_size = tokenizer_info.get('total_vocab_size', 4779)  # Unified vocabulary\n",
        "pad_token_id = tokenizer_info.get('pad_token_id', PAD_TOKEN)\n",
        "\n",
        "model = T5OfflineTeacherModel(\n",
        "    melody_vocab_size=melody_vocab_size,\n",
        "    chord_vocab_size=chord_vocab_size,\n",
        "    embed_dim=config['embed_dim'],\n",
        "    num_heads=config['num_heads'],\n",
        "    num_layers=config['num_layers'],\n",
        "    dropout=config['dropout'],\n",
        "    max_seq_length=config['max_sequence_length'],\n",
        "    pad_token_id=pad_token_id,\n",
        "    total_vocab_size=total_vocab_size  # Use unified vocabulary\n",
        ").to(device)\n",
        "\n",
        "print(f\"Model created with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
        "\n",
        "# Use AdamW optimizer for better stability and simpler hyperparameter tuning\n",
        "optimizer = AdamW(\n",
        "    model.parameters(), \n",
        "    lr=config['learning_rate'],\n",
        "    weight_decay=config.get('weight_decay', 0.01)\n",
        ")\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_token_id)\n",
        "\n",
        "# Initialize warmup scheduler\n",
        "scheduler = get_warmup_schedule(optimizer, num_warmup_steps=config['warmup_steps'])\n",
        "\n",
        "# Enable gradient and parameter logging\n",
        "wandb.watch(model, log=\"all\", log_freq=config['log_every_n_steps'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_epoch(model, train_loader, criterion, optimizer, scheduler, device, global_step, epoch):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    \n",
        "    pbar = tqdm(train_loader, desc='Training')\n",
        "    for batch_idx, batch in enumerate(pbar):\n",
        "        # Move batch to device\n",
        "        melody_tokens = batch['melody_tokens'].to(device)\n",
        "        chord_input = batch['chord_input'].to(device)\n",
        "        chord_target = batch['chord_target'].to(device)\n",
        "        \n",
        "        # Create padding masks (True means position should be masked)\n",
        "        melody_padding_mask = (melody_tokens == model.pad_token_id)  # [batch_size, src_len]\n",
        "        chord_padding_mask = (chord_input == model.pad_token_id)     # [batch_size, tgt_len]\n",
        "        \n",
        "        # Forward pass with proper masking (causal mask is handled internally)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(\n",
        "            melody_tokens=melody_tokens,\n",
        "            chord_tokens=chord_input,\n",
        "            melody_mask=melody_padding_mask,  # src_key_padding_mask\n",
        "            chord_mask=chord_padding_mask     # tgt_key_padding_mask\n",
        "        )\n",
        "        \n",
        "        # Calculate loss\n",
        "        loss = criterion(logits.view(-1, logits.size(-1)), chord_target.view(-1))\n",
        "        \n",
        "        # Check for NaN loss\n",
        "        if torch.isnan(loss):\n",
        "            print(f\"\\nNaN loss detected! Skipping batch.\")\n",
        "            if epoch < 3:  # Early epochs\n",
        "                print(\"NaN in early epoch - may need to reduce learning rate or increase warmup steps\")\n",
        "            continue\n",
        "        \n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), config['gradient_clip_val'])\n",
        "        optimizer.step()\n",
        "        scheduler.step()  # Update learning rate each batch\n",
        "        \n",
        "        # Get current learning rate\n",
        "        lr = optimizer.param_groups[0]['lr']\n",
        "        \n",
        "        # Log batch metrics\n",
        "        if batch_idx % config['log_every_n_steps'] == 0:\n",
        "            wandb.log({\n",
        "                'train/batch_loss': loss.item(),\n",
        "                'train/learning_rate': lr,\n",
        "                'train/batch': batch_idx,\n",
        "                'train/epoch': epoch,\n",
        "                'train/grad_norm': torch.nn.utils.clip_grad_norm_(model.parameters(), float('inf')).item()\n",
        "            }, step=global_step)\n",
        "        \n",
        "        global_step += 1\n",
        "        \n",
        "        # Update progress bar\n",
        "        total_loss += loss.item()\n",
        "        pbar.set_postfix({'loss': loss.item(), 'lr': f\"{lr:.2e}\"})\n",
        "        \n",
        "    return total_loss / len(train_loader), global_step\n",
        "\n",
        "def validate(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    nan_batches = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_loader, desc='Validating'):\n",
        "            # Move batch to device\n",
        "            melody_tokens = batch['melody_tokens'].to(device)\n",
        "            chord_input = batch['chord_input'].to(device)\n",
        "            chord_target = batch['chord_target'].to(device)\n",
        "            \n",
        "            # Create padding masks (True means position should be masked)\n",
        "            melody_padding_mask = (melody_tokens == model.pad_token_id)  # [batch_size, src_len]\n",
        "            chord_padding_mask = (chord_input == model.pad_token_id)     # [batch_size, tgt_len]\n",
        "            \n",
        "            # Forward pass with proper masking (causal mask is handled internally)\n",
        "            logits = model(\n",
        "                melody_tokens=melody_tokens,\n",
        "                chord_tokens=chord_input,\n",
        "                melody_mask=melody_padding_mask,  # src_key_padding_mask\n",
        "                chord_mask=chord_padding_mask     # tgt_key_padding_mask\n",
        "            )\n",
        "            \n",
        "            # Calculate loss\n",
        "            loss = criterion(logits.view(-1, logits.size(-1)), chord_target.view(-1))\n",
        "            \n",
        "            # Check for NaN loss\n",
        "            if torch.isnan(loss):\n",
        "                nan_batches += 1\n",
        "                continue\n",
        "                \n",
        "            total_loss += loss.item()\n",
        "    \n",
        "    # Avoid division by zero if all batches were NaN\n",
        "    num_valid_batches = len(val_loader) - nan_batches\n",
        "    return total_loss / num_valid_batches if num_valid_batches > 0 else float('nan')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training loop\n",
        "best_val_loss = float('inf')\n",
        "global_step = 0\n",
        "\n",
        "try:\n",
        "    for epoch in range(config['max_epochs']):\n",
        "        print(f\"\\nEpoch {epoch + 1}/{config['max_epochs']}\")\n",
        "        \n",
        "        # Clear GPU memory before each epoch\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "            print(f\"GPU memory at start of epoch: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
        "        \n",
        "        # Training Step\n",
        "        train_loss, global_step = train_epoch(model, train_loader, criterion, optimizer, scheduler, device, global_step, epoch)\n",
        "        \n",
        "        # Validation Step\n",
        "        val_loss = validate(model, val_loader, criterion, device)\n",
        "        \n",
        "        print(f\"Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Valid Loss: {val_loss:.4f}\")\n",
        "        wandb.log({\n",
        "            'train/epoch_loss': train_loss,\n",
        "            'valid/epoch_loss': val_loss,\n",
        "            'epoch': epoch + 1,\n",
        "            'train/epoch': epoch + 1\n",
        "        }, step=global_step)\n",
        "        \n",
        "        # Save checkpoint if validation loss improved\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            \n",
        "            # Save checkpoint locally\n",
        "            checkpoint_path = Path(\"checkpoints\") / f\"offline_teacher_epoch_{epoch+1}.pth\"\n",
        "            checkpoint_path.parent.mkdir(exist_ok=True)\n",
        "            \n",
        "            checkpoint = {\n",
        "                'epoch': epoch + 1,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'scheduler_state_dict': scheduler.state_dict(),\n",
        "                'train_loss': train_loss,\n",
        "                'val_loss': val_loss,\n",
        "                'config': config,\n",
        "            }\n",
        "            \n",
        "            torch.save(checkpoint, checkpoint_path)\n",
        "            \n",
        "            # Log checkpoint as wandb artifact\n",
        "            artifact = wandb.Artifact(\n",
        "                name=f\"offline_teacher_model_{wandb.run.id}\",\n",
        "                type=\"model\",\n",
        "                description=f\"Offline Teacher Model checkpoint from epoch {epoch+1}\"\n",
        "            )\n",
        "            artifact.add_file(str(checkpoint_path))\n",
        "            wandb.log_artifact(artifact)\n",
        "            \n",
        "            # Also save tokenizer info as artifact\n",
        "            tokenizer_artifact = wandb.Artifact(\n",
        "                name=f\"tokenizer_info_{wandb.run.id}\",\n",
        "                type=\"tokenizer\",\n",
        "                description=\"Tokenizer information used for training\"\n",
        "            )\n",
        "            tokenizer_path = Path(\"tokenizer_info.json\")\n",
        "            with open(tokenizer_path, 'w') as f:\n",
        "                json.dump(tokenizer_info, f)\n",
        "            tokenizer_artifact.add_file(str(tokenizer_path))\n",
        "            wandb.log_artifact(tokenizer_artifact)\n",
        "            \n",
        "            print(f\"\\nSaved checkpoint with validation loss: {val_loss:.4f}\")\n",
        "            \n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\nTraining interrupted by user\")\n",
        "except torch.cuda.OutOfMemoryError:\n",
        "    print(\"\\nOut of GPU memory! Try reducing batch size or sequence length\")\n",
        "finally:\n",
        "    wandb.finish()\n",
        "    print(\"\\nTraining completed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_best_checkpoint(model, checkpoint_path):\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    return checkpoint['val_loss']\n",
        "\n",
        "# Example usage:\n",
        "# best_checkpoint_path = Path(wandb.run.dir) / 'best_checkpoint.pth'\n",
        "# best_val_loss = load_best_checkpoint(model, best_checkpoint_path)\n",
        "# print(f\"Loaded checkpoint with validation loss: {best_val_loss:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
