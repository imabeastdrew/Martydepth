{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!git clone https://github.com/imabeastdrew/Martydepth.git\n",
        "%cd Martydepth\n",
        "\n",
        "# Install the package in development mode\n",
        "%pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "%pip install torch wandb tqdm pyyaml transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import AdamW\n",
        "from transformers import Adafactor\n",
        "from pathlib import Path\n",
        "import wandb\n",
        "import yaml\n",
        "import json\n",
        "from tqdm.notebook import tqdm\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Add project root to Python path\n",
        "import sys\n",
        "sys.path.append('.')\n",
        "\n",
        "print(os.getcwd())\n",
        "\n",
        "# Import project modules\n",
        "from src.data.dataset import create_dataloader\n",
        "from src.models.offline_teacher import OfflineTeacherModel\n",
        "from src.models.offline_teacher_t5 import T5OfflineTeacherModel\n",
        "from src.config.tokenization_config import PAD_TOKEN, CHORD_TOKEN_START\n",
        "from src.training.utils.schedulers import get_warmup_schedule\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration - Set model type here\n",
        "MODEL_TYPE = \"custom\"  # Change to \"t5\" to use T5OfflineTeacherModel\n",
        "\n",
        "# Load configuration based on model type\n",
        "if MODEL_TYPE == \"custom\":\n",
        "    config_path = 'src/training/configs/offline_teacher_base.yaml'\n",
        "elif MODEL_TYPE == \"t5\":\n",
        "    config_path = 'src/training/configs/offline_teacher_t5.yaml'\n",
        "else:\n",
        "    raise ValueError(f\"Unknown model type: {MODEL_TYPE}. Use 'custom' or 't5'\")\n",
        "\n",
        "print(f\"Loading config from: {config_path}\")\n",
        "with open(config_path, 'r') as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "# Override model type if needed\n",
        "config['model_type'] = MODEL_TYPE\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "print(f'ðŸŽµ Training {MODEL_TYPE.upper()} model')\n",
        "\n",
        "# Initialize wandb with model type in name\n",
        "model_type_for_name = config.get('model_type', 'custom')\n",
        "run_name = (\n",
        "    f\"offline_{model_type_for_name}_L{config['num_layers']}_H{config['num_heads']}\"\n",
        "    f\"_D{config['embed_dim']}_seq{config['max_sequence_length']}\"\n",
        "    f\"_bs{config['batch_size']}_lr{config['learning_rate']}\"\n",
        ")\n",
        "\n",
        "wandb.init(\n",
        "    project=config['wandb_project'],\n",
        "    name=run_name,\n",
        "    config=config,\n",
        "    job_type=\"offline_training\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create dataloaders\n",
        "train_loader, tokenizer_info = create_dataloader(\n",
        "    data_dir=Path(config['data_dir']),\n",
        "    split=\"train\",\n",
        "    batch_size=config['batch_size'],\n",
        "    num_workers=config['num_workers'],\n",
        "    sequence_length=config['max_sequence_length'],\n",
        "    mode='offline',\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_loader, _ = create_dataloader(\n",
        "    data_dir=Path(config['data_dir']),\n",
        "    split=\"valid\",\n",
        "    batch_size=config['batch_size'],\n",
        "    num_workers=config['num_workers'],\n",
        "    sequence_length=config['max_sequence_length'],\n",
        "    mode='offline',\n",
        "    shuffle=False\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize model, optimizer, criterion, and scheduler\n",
        "config['melody_vocab_size'] = tokenizer_info['melody_vocab_size']\n",
        "config['chord_vocab_size'] = tokenizer_info['chord_vocab_size']\n",
        "config['total_vocab_size'] = tokenizer_info['total_vocab_size']\n",
        "\n",
        "# --- Model Creation (Configurable) ---\n",
        "def create_model(model_type: str):\n",
        "    \"\"\"Create model based on configuration\"\"\"\n",
        "    if model_type == \"custom\":\n",
        "        return OfflineTeacherModel(\n",
        "            melody_vocab_size=config['melody_vocab_size'],\n",
        "            chord_vocab_size=config['chord_vocab_size'],\n",
        "            embed_dim=config['embed_dim'],\n",
        "            num_heads=config['num_heads'],\n",
        "            num_layers=config['num_layers'],\n",
        "            dropout=config['dropout'],\n",
        "            max_seq_length=config['max_sequence_length'],\n",
        "            pad_token_id=tokenizer_info.get('pad_token_id', PAD_TOKEN)\n",
        "        )\n",
        "    elif model_type == \"t5\":\n",
        "        return T5OfflineTeacherModel(\n",
        "            melody_vocab_size=config['melody_vocab_size'],\n",
        "            chord_vocab_size=config['chord_vocab_size'],\n",
        "            embed_dim=config['embed_dim'],\n",
        "            num_heads=config['num_heads'],\n",
        "            num_layers=config['num_layers'],\n",
        "            dropout=config['dropout'],\n",
        "            max_seq_length=config['max_sequence_length'],\n",
        "            pad_token_id=tokenizer_info.get('pad_token_id', PAD_TOKEN),\n",
        "            total_vocab_size=config['total_vocab_size']\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model_type: {model_type}. Use 'custom' or 't5'\")\n",
        "\n",
        "model_type = config.get('model_type', 'custom')  # Default to custom model\n",
        "model = create_model(model_type).to(device)\n",
        "\n",
        "print(f\"ðŸŽµ Using {model_type.upper()} model architecture\")\n",
        "print(f\"Model created with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
        "\n",
        "# Use AdamW optimizer for better stability and simpler hyperparameter tuning\n",
        "optimizer = AdamW(\n",
        "    model.parameters(), \n",
        "    lr=config['learning_rate'],\n",
        "    weight_decay=config.get('weight_decay', 0.01)\n",
        ")\n",
        "\n",
        "# Note: Cross-entropy loss will be calculated in the training function with proper vocab sizing\n",
        "pad_token_id = tokenizer_info.get('pad_token_id', PAD_TOKEN)\n",
        "\n",
        "# Initialize warmup scheduler\n",
        "scheduler = get_warmup_schedule(optimizer, num_warmup_steps=config['warmup_steps'])\n",
        "\n",
        "# Enable gradient and parameter logging\n",
        "wandb.watch(model, log=\"all\", log_freq=config['log_every_n_steps'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_epoch(model, train_loader, optimizer, scheduler, device, global_step, epoch, model_type, config):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    \n",
        "    pbar = tqdm(train_loader, desc='Training')\n",
        "    for batch_idx, batch in enumerate(pbar):\n",
        "        # Move batch to device\n",
        "        melody_tokens = batch['melody_tokens'].to(device)\n",
        "        chord_input = batch['chord_input'].to(device)\n",
        "        chord_target = batch['chord_target'].to(device)\n",
        "        \n",
        "        # Create padding masks (True means position should be masked)\n",
        "        melody_padding_mask = (melody_tokens == model.pad_token_id)  # [batch_size, src_len]\n",
        "        chord_padding_mask = (chord_input == model.pad_token_id)     # [batch_size, tgt_len]\n",
        "        \n",
        "        # Forward pass with proper masking (causal mask is handled internally)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(\n",
        "            melody_tokens=melody_tokens,\n",
        "            chord_tokens=chord_input,\n",
        "            melody_mask=melody_padding_mask,  # src_key_padding_mask\n",
        "            chord_mask=chord_padding_mask     # tgt_key_padding_mask\n",
        "        )\n",
        "        \n",
        "        # Use chord vocab size for both models to isolate architectural differences\n",
        "        vocab_size_for_loss = config['chord_vocab_size']\n",
        "        \n",
        "        # Extract chord-only logits for T5 model (which outputs full vocab)\n",
        "        if model_type == \"t5\":\n",
        "            # Chord tokens start at CHORD_TOKEN_START (179) in the full vocabulary\n",
        "            chord_logits = logits[:, :, CHORD_TOKEN_START:]  # [batch, seq, chord_vocab_size]\n",
        "            logits_for_loss = chord_logits\n",
        "            \n",
        "            # Adjust targets from full vocab space to chord-only space\n",
        "            # Original targets: [179, 180, ..., 4778] -> [0, 1, ..., 4599]\n",
        "            targets_for_loss = chord_target - CHORD_TOKEN_START\n",
        "            # Handle PAD tokens (they should remain as pad_token_id for ignore_index)\n",
        "            pad_mask = (chord_target == model.pad_token_id)\n",
        "            targets_for_loss[pad_mask] = model.pad_token_id\n",
        "        else:  # custom model already outputs chord-only\n",
        "            logits_for_loss = logits\n",
        "            targets_for_loss = chord_target  # Already in chord space\n",
        "            \n",
        "        # Calculate loss\n",
        "        loss = nn.functional.cross_entropy(\n",
        "            logits_for_loss.reshape(-1, vocab_size_for_loss),\n",
        "            targets_for_loss.reshape(-1),\n",
        "            ignore_index=model.pad_token_id\n",
        "        )\n",
        "        \n",
        "        # Check for NaN loss\n",
        "        if torch.isnan(loss):\n",
        "            print(f\"\\nNaN loss detected! Skipping batch.\")\n",
        "            if epoch < 3:  # Early epochs\n",
        "                print(\"NaN in early epoch - may need to reduce learning rate or increase warmup steps\")\n",
        "            continue\n",
        "        \n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), config['gradient_clip_val'])\n",
        "        optimizer.step()\n",
        "        scheduler.step()  # Update learning rate each batch\n",
        "        \n",
        "        # Get current learning rate\n",
        "        lr = optimizer.param_groups[0]['lr']\n",
        "        \n",
        "        # Log batch metrics\n",
        "        if batch_idx % config['log_every_n_steps'] == 0:\n",
        "            wandb.log({\n",
        "                'train/batch_loss': loss.item(),\n",
        "                'train/learning_rate': lr,\n",
        "                'train/batch': batch_idx,\n",
        "                'train/epoch': epoch,\n",
        "                'train/grad_norm': torch.nn.utils.clip_grad_norm_(model.parameters(), float('inf')).item()\n",
        "            }, step=global_step)\n",
        "        \n",
        "        global_step += 1\n",
        "        \n",
        "        # Update progress bar\n",
        "        total_loss += loss.item()\n",
        "        pbar.set_postfix({'loss': loss.item(), 'lr': f\"{lr:.2e}\"})\n",
        "        \n",
        "    return total_loss / len(train_loader), global_step\n",
        "\n",
        "def validate(model, val_loader, device, model_type, config):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    nan_batches = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_loader, desc='Validating'):\n",
        "            # Move batch to device\n",
        "            melody_tokens = batch['melody_tokens'].to(device)\n",
        "            chord_input = batch['chord_input'].to(device)\n",
        "            chord_target = batch['chord_target'].to(device)\n",
        "            \n",
        "            # Create padding masks (True means position should be masked)\n",
        "            melody_padding_mask = (melody_tokens == model.pad_token_id)  # [batch_size, src_len]\n",
        "            chord_padding_mask = (chord_input == model.pad_token_id)     # [batch_size, tgt_len]\n",
        "            \n",
        "            # Forward pass with proper masking (causal mask is handled internally)\n",
        "            logits = model(\n",
        "                melody_tokens=melody_tokens,\n",
        "                chord_tokens=chord_input,\n",
        "                melody_mask=melody_padding_mask,  # src_key_padding_mask\n",
        "                chord_mask=chord_padding_mask     # tgt_key_padding_mask\n",
        "            )\n",
        "            \n",
        "            # Use chord vocab size for both models to isolate architectural differences\n",
        "            vocab_size_for_loss = config['chord_vocab_size']\n",
        "            \n",
        "            # Extract chord-only logits for T5 model (which outputs full vocab)\n",
        "            if model_type == \"t5\":\n",
        "                # Chord tokens start at CHORD_TOKEN_START (179) in the full vocabulary\n",
        "                chord_logits = logits[:, :, CHORD_TOKEN_START:]  # [batch, seq, chord_vocab_size]\n",
        "                logits_for_loss = chord_logits\n",
        "                \n",
        "                # Adjust targets from full vocab space to chord-only space\n",
        "                # Original targets: [179, 180, ..., 4778] -> [0, 1, ..., 4599]\n",
        "                targets_for_loss = chord_target - CHORD_TOKEN_START\n",
        "                # Handle PAD tokens (they should remain as pad_token_id for ignore_index)\n",
        "                pad_mask = (chord_target == model.pad_token_id)\n",
        "                targets_for_loss[pad_mask] = model.pad_token_id\n",
        "            else:  # custom model already outputs chord-only\n",
        "                logits_for_loss = logits\n",
        "                targets_for_loss = chord_target  # Already in chord space\n",
        "                \n",
        "            # Calculate loss\n",
        "            loss = nn.functional.cross_entropy(\n",
        "                logits_for_loss.reshape(-1, vocab_size_for_loss),\n",
        "                targets_for_loss.reshape(-1),\n",
        "                ignore_index=model.pad_token_id\n",
        "            )\n",
        "            \n",
        "            # Check for NaN loss\n",
        "            if torch.isnan(loss):\n",
        "                nan_batches += 1\n",
        "                continue\n",
        "                \n",
        "            total_loss += loss.item()\n",
        "    \n",
        "    # Avoid division by zero if all batches were NaN\n",
        "    num_valid_batches = len(val_loader) - nan_batches\n",
        "    return total_loss / num_valid_batches if num_valid_batches > 0 else float('nan')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training loop\n",
        "best_val_loss = float('inf')\n",
        "global_step = 0\n",
        "\n",
        "print(f\"\\n--- Offline Training Info ---\")\n",
        "print(f\"  Model type: {model_type}\")\n",
        "print(f\"  Max epochs: {config['max_epochs']}\")\n",
        "print(f\"  Early stopping patience: {config.get('early_stopping_patience', 5)}\")\n",
        "\n",
        "try:\n",
        "    for epoch in range(config['max_epochs']):\n",
        "        print(f\"\\nEpoch {epoch + 1}/{config['max_epochs']}\")\n",
        "        \n",
        "        # Clear GPU memory before each epoch\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "            print(f\"GPU memory at start of epoch: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
        "        \n",
        "        # Training Step\n",
        "        train_loss, global_step = train_epoch(model, train_loader, optimizer, scheduler, device, global_step, epoch, model_type, config)\n",
        "        \n",
        "        # Validation Step\n",
        "        val_loss = validate(model, val_loader, device, model_type, config)\n",
        "        \n",
        "        print(f\"Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Valid Loss: {val_loss:.4f}\")\n",
        "        wandb.log({\n",
        "            'train/epoch_loss': train_loss,\n",
        "            'valid/epoch_loss': val_loss,\n",
        "            'epoch': epoch + 1,\n",
        "            'train/epoch': epoch + 1\n",
        "        }, step=global_step)\n",
        "        \n",
        "        # Save checkpoint if validation loss improved\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            \n",
        "            # Save checkpoint locally\n",
        "            checkpoint_path = Path(\"checkpoints\") / f\"offline_teacher_epoch_{epoch+1}.pth\"\n",
        "            checkpoint_path.parent.mkdir(exist_ok=True)\n",
        "            \n",
        "            checkpoint = {\n",
        "                'epoch': epoch + 1,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'scheduler_state_dict': scheduler.state_dict(),\n",
        "                'train_loss': train_loss,\n",
        "                'val_loss': val_loss,\n",
        "                'config': config,\n",
        "            }\n",
        "            \n",
        "            torch.save(checkpoint, checkpoint_path)\n",
        "            \n",
        "            # Log checkpoint as wandb artifact\n",
        "            artifact = wandb.Artifact(\n",
        "                name=f\"offline_teacher_model_{wandb.run.id}\",\n",
        "                type=\"model\",\n",
        "                description=f\"Offline Teacher Model checkpoint from epoch {epoch+1}\"\n",
        "            )\n",
        "            artifact.add_file(str(checkpoint_path))\n",
        "            wandb.log_artifact(artifact)\n",
        "            \n",
        "            # Also save tokenizer info as artifact\n",
        "            tokenizer_artifact = wandb.Artifact(\n",
        "                name=f\"tokenizer_info_{wandb.run.id}\",\n",
        "                type=\"tokenizer\",\n",
        "                description=\"Tokenizer information used for training\"\n",
        "            )\n",
        "            tokenizer_path = Path(\"tokenizer_info.json\")\n",
        "            with open(tokenizer_path, 'w') as f:\n",
        "                json.dump(tokenizer_info, f)\n",
        "            tokenizer_artifact.add_file(str(tokenizer_path))\n",
        "            wandb.log_artifact(tokenizer_artifact)\n",
        "            \n",
        "            print(f\"\\nSaved checkpoint with validation loss: {val_loss:.4f}\")\n",
        "            \n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\nTraining interrupted by user\")\n",
        "except torch.cuda.OutOfMemoryError:\n",
        "    print(\"\\nOut of GPU memory! Try reducing batch size or sequence length\")\n",
        "finally:\n",
        "    wandb.finish()\n",
        "    print(\"\\nTraining completed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_best_checkpoint(model, checkpoint_path):\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    return checkpoint['val_loss']\n",
        "\n",
        "# Example usage:\n",
        "# best_checkpoint_path = Path(wandb.run.dir) / 'best_checkpoint.pth'\n",
        "# best_val_loss = load_best_checkpoint(model, best_checkpoint_path)\n",
        "# print(f\"Loaded checkpoint with validation loss: {best_val_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model Comparison Analysis (Optional)\n",
        "def analyze_model_performance():\n",
        "    \"\"\"\n",
        "    Compare model architectures and performance metrics\n",
        "    \"\"\"\n",
        "    print(\"ðŸŽµ Model Architecture Comparison\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    print(f\"Selected Model Type: {model_type.upper()}\")\n",
        "    print(f\"Total Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "    \n",
        "    if model_type == \"custom\":\n",
        "        print(\"âœ… Custom Model Advantages:\")\n",
        "        print(\"  - Native chord-only output head\")\n",
        "        print(\"  - 100% parameter efficiency\")\n",
        "        print(\"  - Task-specific architecture\")\n",
        "        print(\"  - No vocabulary confusion\")\n",
        "        print(f\"  - Output shape: [batch, seq, {config['chord_vocab_size']}]\")\n",
        "    \n",
        "    elif model_type == \"t5\":\n",
        "        print(\"âŒ T5 Model Issues:\")\n",
        "        print(\"  - Unified vocabulary (melody + chord)\")\n",
        "        print(\"  - 3.8% wasted parameters (unused melody outputs)\")\n",
        "        print(\"  - Text-to-text design for cross-domain task\")\n",
        "        print(\"  - Requires logit extraction + target adjustment\")\n",
        "        print(f\"  - Output shape: [batch, seq, {config['total_vocab_size']}] â†’ extract chord portion\")\n",
        "        \n",
        "        # Calculate parameter waste\n",
        "        total_params = config['total_vocab_size'] * config['embed_dim']\n",
        "        wasted_params = 179 * config['embed_dim']  # Melody + PAD tokens\n",
        "        waste_percent = (wasted_params / total_params) * 100\n",
        "        print(f\"  - Output head waste: {wasted_params:,} / {total_params:,} ({waste_percent:.1f}%)\")\n",
        "\n",
        "# Run analysis\n",
        "analyze_model_performance()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
