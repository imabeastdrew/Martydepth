{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Offline Teacher Model Training\n",
        "\n",
        "This notebook provides a streamlined workflow for training the offline teacher model for melody-to-chord generation.\n",
        "\n",
        "## Setup and Configuration\n",
        "First, we'll install dependencies and set up our environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "%pip install torch wandb tqdm pyyaml\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone the repository if running on Colab\n",
        "import os\n",
        "if not os.path.exists('Martydepth'):\n",
        "    !git clone https://github.com/yourusername/Martydepth.git\n",
        "    %cd Martydepth\n",
        "else:\n",
        "    %cd Martydepth\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "from pathlib import Path\n",
        "import wandb\n",
        "import yaml\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Add project root to Python path\n",
        "import sys\n",
        "sys.path.append('.')\n",
        "\n",
        "# Import project modules\n",
        "from src.data.dataset import create_dataloader\n",
        "from src.models.offline_teacher import OfflineTeacherModel\n",
        "from src.config.tokenization_config import PAD_TOKEN\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Configuration\n",
        "Load the model configuration and set up training parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load configuration\n",
        "config_path = 'src/training/configs/offline_teacher_base.yaml'\n",
        "with open(config_path, 'r') as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "# Initialize wandb\n",
        "run_name = f\"offline_L{config['num_layers']}_H{config['num_heads']}_D{config['embed_dim']}_seq{config['max_sequence_length']}_bs{config['batch_size']}_lr{config['learning_rate']}\"\n",
        "\n",
        "wandb.init(\n",
        "    project=config['wandb_project'],\n",
        "    name=run_name,\n",
        "    config=config,\n",
        "    job_type=\"offline_training\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Data Loading\n",
        "Create data loaders for training and validation sets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create dataloaders\n",
        "train_loader, tokenizer_info = create_dataloader(\n",
        "    data_dir=Path(config['data_dir']),\n",
        "    split=\"train\",\n",
        "    batch_size=config['batch_size'],\n",
        "    num_workers=config['num_workers'],\n",
        "    sequence_length=config['max_sequence_length'],\n",
        "    mode='offline',\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_loader, _ = create_dataloader(\n",
        "    data_dir=Path(config['data_dir']),\n",
        "    split=\"valid\",\n",
        "    batch_size=config['batch_size'],\n",
        "    num_workers=config['num_workers'],\n",
        "    sequence_length=config['max_sequence_length'],\n",
        "    mode='offline',\n",
        "    shuffle=False\n",
        ")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Model Setup\n",
        "Initialize the model, loss function, optimizer, and learning rate scheduler.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize model\n",
        "model = OfflineTeacherModel(\n",
        "    melody_vocab_size=tokenizer_info['melody_vocab_size'],\n",
        "    chord_vocab_size=tokenizer_info['chord_vocab_size'],\n",
        "    embed_dim=config['embed_dim'],\n",
        "    num_heads=config['num_heads'],\n",
        "    num_layers=config['num_layers'],\n",
        "    dropout=config['dropout'],\n",
        "    max_seq_length=config['max_sequence_length'],\n",
        "    pad_token_id=tokenizer_info.get('pad_token_id', PAD_TOKEN)\n",
        ").to(device)\n",
        "\n",
        "# Loss function\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=model.pad_token_id)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = Adam(model.parameters(), lr=config['learning_rate'])\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer,\n",
        "    mode='min',\n",
        "    factor=0.5,\n",
        "    patience=5,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "print(f\"Model created with {sum(p.numel() for p in model.parameters()):,} parameters.\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Training Loop\n",
        "Define training and validation functions, then run the training loop.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    \n",
        "    pbar = tqdm(train_loader, desc='Training')\n",
        "    for batch in pbar:\n",
        "        # Move batch to device\n",
        "        melody_tokens = batch['melody_tokens'].to(device)\n",
        "        chord_input = batch['chord_input'].to(device)\n",
        "        chord_target = batch['chord_target'].to(device)\n",
        "        \n",
        "        # Create masks\n",
        "        melody_mask = (melody_tokens == model.pad_token_id)\n",
        "        chord_mask = (chord_input == model.pad_token_id)\n",
        "        \n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(melody_tokens, chord_input, melody_mask, chord_mask)\n",
        "        \n",
        "        # Calculate loss\n",
        "        loss = criterion(logits.view(-1, logits.size(-1)), chord_target.view(-1))\n",
        "        \n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Update progress bar\n",
        "        total_loss += loss.item()\n",
        "        pbar.set_postfix({'loss': loss.item()})\n",
        "        \n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "def validate(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_loader, desc='Validating'):\n",
        "            # Move batch to device\n",
        "            melody_tokens = batch['melody_tokens'].to(device)\n",
        "            chord_input = batch['chord_input'].to(device)\n",
        "            chord_target = batch['chord_target'].to(device)\n",
        "            \n",
        "            # Create masks\n",
        "            melody_mask = (melody_tokens == model.pad_token_id)\n",
        "            chord_mask = (chord_input == model.pad_token_id)\n",
        "            \n",
        "            # Forward pass\n",
        "            logits = model(melody_tokens, chord_input, melody_mask, chord_mask)\n",
        "            \n",
        "            # Calculate loss\n",
        "            loss = criterion(logits.view(-1, logits.size(-1)), chord_target.view(-1))\n",
        "            total_loss += loss.item()\n",
        "            \n",
        "    return total_loss / len(val_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training loop\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "try:\n",
        "    for epoch in range(config['epochs']):\n",
        "        print(f\"\\nEpoch {epoch + 1}/{config['epochs']}\")\n",
        "        \n",
        "        # Training\n",
        "        train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "        \n",
        "        # Validation\n",
        "        val_loss = validate(model, val_loader, criterion, device)\n",
        "        \n",
        "        # Update learning rate\n",
        "        scheduler.step(val_loss)\n",
        "        \n",
        "        # Log metrics\n",
        "        wandb.log({\n",
        "            'train/loss': train_loss,\n",
        "            'valid/loss': val_loss,\n",
        "            'learning_rate': optimizer.param_groups[0]['lr'],\n",
        "            'epoch': epoch + 1\n",
        "        })\n",
        "        \n",
        "        # Save checkpoint if validation loss improved\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            checkpoint_path = Path(wandb.run.dir) / f\"offline_teacher_epoch_{epoch+1}.pth\"\n",
        "            \n",
        "            torch.save({\n",
        "                'epoch': epoch + 1,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'scheduler_state_dict': scheduler.state_dict(),\n",
        "                'train_loss': train_loss,\n",
        "                'val_loss': val_loss,\n",
        "                'config': config,\n",
        "            }, checkpoint_path)\n",
        "            \n",
        "            wandb.save(str(checkpoint_path))\n",
        "            print(f\"\\nSaved checkpoint with validation loss: {val_loss:.4f}\")\n",
        "            \n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\nTraining interrupted by user\")\n",
        "finally:\n",
        "    wandb.finish()\n",
        "    print(\"\\nTraining completed\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Model Evaluation\n",
        "After training, you can load the best checkpoint and evaluate the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_best_checkpoint(model, checkpoint_path):\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    return checkpoint['val_loss']\n",
        "\n",
        "# Example usage:\n",
        "# best_checkpoint_path = Path(wandb.run.dir) / 'best_checkpoint.pth'\n",
        "# best_val_loss = load_best_checkpoint(model, best_checkpoint_path)\n",
        "# print(f\"Loaded checkpoint with validation loss: {best_val_loss:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
